{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Selective Search</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n",
    "  \n",
    "  \n",
    "<img src=\"asset/7_1/main.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook addresses the problem of ***generating possible object locations*** for use in object recognition. We introduce ***Selective Search***\n",
    "which combines the strength of both an ***exhaustive search*** and ***segmentation***. Like segmentation, we use the image structure to guide\n",
    "our sampling process. Like exhaustive search, we aim to capture\n",
    "all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a\n",
    "variety of complementary image partitionings to deal with as many\n",
    "image conditions as possible. Our Selective Search results in a\n",
    "small set of data-driven, class-independent, high quality locations,\n",
    "yielding 99% recall and a Mean Average Best Overlap of 0.879 at\n",
    "10,097 locations. The reduced number of locations compared to\n",
    "an exhaustive search enables the use of stronger machine learning\n",
    "techniques and stronger appearance models for object recognition.\n",
    "In this notebook we show that selective search enables the use of\n",
    "the powerful ***Bag-of-Words model for recognition***. The Selective\n",
    "Search software is made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">1 Introduction</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a long time, objects were sought to be delineated before their\n",
    "identification. This gave rise to segmentation, which aims for\n",
    "a unique partitioning of the image through a generic algorithm,\n",
    "where there is one part for all object silhouettes in the image. Research on this topic has yielded tremendous progress over the past\n",
    "years. But images are intrinsically hierarchical: In\n",
    "Figure 1a the salad and spoons are inside the salad bowl, which in\n",
    "turn stands on the table. Furthermore, depending on the context the\n",
    "term table in this picture can refer to only the wood or include everything on the table. Therefore both the nature of images and the\n",
    "different uses of an object category are hierarchical. This prohibits\n",
    "the unique partitioning of objects for all but the most specific purposes. Hence for most tasks multiple scales in a segmentation are a\n",
    "necessity. This is most naturally addressed by using a hierarchical\n",
    "partitioning, as done for example by Arbelaez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides that a segmentation should be hierarchical, a generic solution for segmentation using a single strategy may not exist at all.\n",
    "There are many conflicting reasons why a region should be grouped\n",
    "together: In Figure 1b the cats can be separated using colour, but\n",
    "their texture is the same. Conversely, in Figure 1c the chameleon is similar to its surrounding leaves in terms of colour, yet its texture differs. Finally, in Figure 1d, the wheels are wildly different\n",
    "from the car in terms of both colour and texture, yet are enclosed\n",
    "by the car. Individual visual features therefore cannot resolve the\n",
    "ambiguity of segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_1/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: There is a high variety of reasons that an image region\n",
    "forms an object. In (b) the cats can be distinguished by colour, not\n",
    "texture. In (c) the chameleon can be distinguished from the surrounding leaves by texture, not colour. In (d) the wheels can be part\n",
    "of the car because they are enclosed, not because they are similar\n",
    "in texture or colour. Therefore, to find objects in a structured way\n",
    "it is necessary to use a variety of diverse strategies. Furthermore,\n",
    "an image is intrinsically hierarchical as there is no single scale for\n",
    "which the complete table, salad bowl, and salad spoon can be found\n",
    "in (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, there is a more fundamental problem. Regions with\n",
    "very different characteristics, such as a face over a sweater, can\n",
    "only be combined into one object after it has been established that\n",
    "the object at hand is a human. Hence without prior recognition it is\n",
    "hard to decide that a face and a sweater are part of one object.\n",
    "This has led to the opposite of the traditional approach: to do\n",
    "localisation through the identification of an object. This recent approach in object recognition has made enormous progress in less\n",
    "than a decade. With an appearance model learned\n",
    "from examples, an exhaustive search is performed where every location within the image is examined as to not miss any potential\n",
    "object location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the exhaustive search itself has several drawbacks.\n",
    "Searching every possible location is computationally infeasible.\n",
    "The search space has to be reduced by using a regular grid, fixed\n",
    "scales, and fixed aspect ratios. In most cases the number of locations to visit remains huge, so much that alternative restrictions\n",
    "need to be imposed. The classifier is simplified and the appearance\n",
    "model needs to be fast. Furthermore, a uniform sampling yields\n",
    "many boxes for which it is immediately clear that they are not supportive of an object. Rather then sampling locations blindly using\n",
    "an exhaustive search, a key question is: Can we steer the sampling\n",
    "by a data-driven analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to combine the best of the intuitions of segmentation and exhaustive search and propose a data-driven selective search. Inspired by bottom-up segmentation, we aim to exploit\n",
    "the structure of the image to generate object locations. Inspired by\n",
    "exhaustive search, we aim to capture all possible object locations.\n",
    "Therefore, instead of using a single sampling technique, we aim\n",
    "to diversify the sampling techniques to account for as many image\n",
    "conditions as possible. Specifically, we use a data-driven groupingbased strategy where we increase diversity by using a variety of\n",
    "complementary grouping criteria and a variety of complementary\n",
    "colour spaces with different invariance properties. The set of locations is obtained by combining the locations of these complementary partitionings. Our goal is to generate a class-independent,\n",
    "data-driven, selective search strategy that generates a small set of\n",
    "high-quality object locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our application domain of selective search is object recognition.\n",
    "We therefore evaluate on the most commonly used dataset for this\n",
    "purpose, the Pascal VOC detection challenge which consists of 20\n",
    "object classes. The size of this dataset yields computational constraints for our selective search. Furthermore, the use of this dataset\n",
    "means that the quality of locations is mainly evaluated in terms of\n",
    "bounding boxes. However, our selective search applies to regions\n",
    "as well and is also applicable to concepts such as “grass”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we propose selective search for object recognition.\n",
    "Our main research questions are: \n",
    "* What are good diversification strategies for adapting segmentation as a selective search strategy?\n",
    "\n",
    "\n",
    "* How effective is selective search in creating a small set of highquality locations within an image? \n",
    "\n",
    "\n",
    "* Can we use selective search to employ more powerful classifiers and appearance models for object recognition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">2 Related Work</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confine the related work to the domain of object recognition\n",
    "and divide it into three categories: \n",
    "\n",
    "* Exhaustive search, \n",
    "\n",
    "\n",
    "* segmentation, and \n",
    "\n",
    "\n",
    "* other sampling strategies that do not fall in either category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">2.1 Exhaustive Search</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an object can be located at any position and scale in the image,\n",
    "it is natural to search everywhere. However, the visual\n",
    "search space is huge, making an exhaustive search computationally\n",
    "expensive. This imposes constraints on the evaluation cost per location and/or the number of locations considered. Hence most of\n",
    "these sliding window techniques use a coarse search grid and fixed\n",
    "aspect ratios, using weak classifiers and economic image features such as HOG (Histogram of oriented gradients). This method is often used as a preselection step in a cascade of classifiers.\n",
    "\n",
    "Related to the sliding window technique is the highly successful\n",
    "part-based object localisation method of Felzenszwalb.\n",
    "Their method also performs an exhaustive search using a linear\n",
    "SVM and HOG features. However, they search for objects and\n",
    "object parts, whose combination results in an impressive object detection performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lampert proposed using the appearance model to\n",
    "guide the search. This both alleviates the constraints of using a\n",
    "regular grid, fixed scales, and fixed aspect ratio, while at the same\n",
    "time reduces the number of locations visited. This is done by directly searching for the optimal window within the image using a\n",
    "branch and bound technique. While they obtain impressive results\n",
    "for linear classifiers, found that for non-linear classifiers the\n",
    "method in practice still visits over a 100,000 windows per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a blind exhaustive search or a branch and bound\n",
    "search, we propose selective search. We use the underlying image structure to generate object locations. In contrast to the discussed methods, this yields a completely class-independent set of\n",
    "locations. Furthermore, because we do not use a fixed aspect ratio, our method is not limited to objects but should be able to find\n",
    "stuff like “grass” and “sand” as well. Finally, we hope to generate fewer locations, which should make the\n",
    "problem easier as the variability of samples becomes lower. And\n",
    "more importantly, it frees up computational power which can be\n",
    "used for stronger machine learning techniques and more powerful\n",
    "appearance models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">2.2 Segmentation</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ***Carreira and Sminchisescu*** and ***Endres and Hoiem*** propose to generate a set of class independent object hypotheses using\n",
    "segmentation. Both methods generate multiple foreground/background segmentations, learn to predict the likelihood that a foreground segment is a complete object, and use this to rank the segments. Both algorithms show a promising ability to accurately\n",
    "delineate objects within images, who achieve\n",
    "state-of-the-art results on pixel-wise image classification.\n",
    "As common in segmentation, both methods rely on a single strong\n",
    "algorithm for identifying good regions. They obtain a variety of\n",
    "locations by using many randomly initialised foreground and background seeds. In contrast, we explicitly deal with a variety of image\n",
    "conditions by using different grouping criteria and different representations. This means a lower computational investment as we do\n",
    "not have to invest in the single best segmentation strategy, such as\n",
    "using the excellent yet expensive contour detector of [3]. Furthermore, as we deal with different image conditions separately, we\n",
    "expect our locations to have a more consistent quality. Finally, our\n",
    "selective search paradigm dictates that the most interesting question is not how our regions compare to, but rather how they\n",
    "can complement each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gu address the problem of carefully segmenting and\n",
    "recognizing objects based on their parts. They first generate a set\n",
    "of part hypotheses using a grouping method based on Arbelaez. Each part hypothesis is described by both appearance and\n",
    "shape features. Then, an object is recognized and carefully delineated by using its parts, achieving good results for shape recognition. In their work, the segmentation is hierarchical and yields segments at all scales. However, they use a single grouping strategy whose power of discovering parts or objects is left unevaluated. In\n",
    "this work, we use multiple complementary strategies to deal with\n",
    "as many image conditions as possible. We include the locations\n",
    "generated in our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">2.3 Other Sampling Strategies</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexe address the problem of the large sampling space\n",
    "of an exhaustive search by proposing to search for any object, independent of its class. In their method they train a classifier on the\n",
    "object windows of those objects which have a well-defined shape\n",
    "(as opposed to stuff like “grass” and “sand”). Then instead of a full\n",
    "exhaustive search they randomly sample boxes to which they apply\n",
    "their classifier. The boxes with the highest “objectness” measure\n",
    "serve as a set of object hypotheses. This set is then used to greatly\n",
    "reduce the number of windows evaluated by class-specific object\n",
    "detectors. We compare our method with their work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is to use visual words of the Bag-of-Words\n",
    "model to predict the object location. Vedaldi use jumping\n",
    "windows, in which the relation between individual visual words\n",
    "and the object location is learned to predict the object location in\n",
    "new images. Maji and Malik combine multiple of these relations to predict the object location using a Hough-transform, after\n",
    "which they randomly sample windows close to the Hough maximum. In contrast to learning, we use the image structure to sample\n",
    "a set of class-independent object hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, our novelty is as follows. Instead of an exhaustive search we use segmentation as selective search\n",
    "yielding a small set of class independent object locations. In contrast to the segmentation, instead of focusing on the best\n",
    "segmentation algorithm, we use a variety of strategies to deal\n",
    "with as many image conditions as possible, thereby severely reducing computational costs while potentially capturing more objects\n",
    "accurately. Instead of learning an objectness measure on randomly\n",
    "sampled boxes, we use a bottom-up grouping procedure to generate good object locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">3 Selective Search</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we detail the selective search algorithm for object\n",
    "recognition and present a variety of diversification strategies to deal\n",
    "with as many image conditions as possible. A selective search algorithm is subject to the following design considerations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_1/2.png'>\n",
    "\n",
    "Figure 2: Two examples of our selective search showing the necessity of different scales. On the left we find many objects at different\n",
    "scales. On the right we necessarily find the objects at different scales as the girl is contained by the tv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Capture All Scales***. Objects can occur at any scale within the image. Furthermore, some objects have less clear boundaries\n",
    "then other objects. Therefore, in selective search all object\n",
    "scales have to be taken into account, as illustrated in Figure2. This is most naturally achieved by using an hierarchical\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Diversification***. There is no single optimal strategy to group regions together. As observed earlier in Figure 1, regions may\n",
    "form an object because of only colour, only texture, or because\n",
    "parts are enclosed. Furthermore, lighting conditions such as\n",
    "shading and the colour of the light may influence how regions\n",
    "form an object. Therefore instead of a single strategy which\n",
    "works well in most cases, we want to have a diverse set of\n",
    "strategies to deal with all cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fast to Compute***. The goal of selective search is to yield a set of\n",
    "possible object locations for use in a practical object recognition framework. The creation of this set should not become a\n",
    "computational bottleneck, hence our algorithm should be reasonably fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">3.1 Selective Search by Hierarchical Grouping</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a hierarchical grouping algorithm to form the basis of our\n",
    "selective search. Bottom-up grouping is a popular approach to segmentation, hence we adapt it for selective search. Because\n",
    "the process of grouping itself is hierarchical, we can naturally generate locations at all scales by continuing the grouping process until\n",
    "the whole image becomes a single region. This satisfies the condition of capturing all scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regions can yield richer information than pixels, we want to\n",
    "use region-based features whenever possible. To get a set of small\n",
    "starting regions which ideally do not span multiple objects, we use the fast method of Felzenszwalb and Huttenlocher, which\n",
    "found well-suited for such purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grouping procedure now works as follows. We first use ***Felzenszwalb and D. P. Huttenlocher. Efficient GraphBased Image Segmentation*** to create initial regions, Then we use a greedy algorithm to iteratively group regions together: First the similarities between all\n",
    "neighbouring regions are calculated. The two most similar regions\n",
    "are grouped together, and new similarities are calculated between\n",
    "the resulting region and its neighbours. The process of grouping\n",
    "the most similar regions is repeated until the whole image becomes\n",
    "a single region. The general method is detailed in Algorithm 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_1/3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the similarity s(ri\n",
    ",rj) between region ri and rj we want a variety of complementary measures under the constraint that they are\n",
    "fast to compute. In effect, this means that the similarities should be\n",
    "based on features that can be propagated through the hierarchy, i.e.\n",
    "when merging region ri and rj\n",
    "into rt\n",
    ", the features of region rt need\n",
    "to be calculated from the features of ri and rj without accessing the\n",
    "image pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">3.2 Diversification Strategies</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second design criterion for selective search is to diversify the\n",
    "sampling and create a set of complementary strategies whose locations are combined afterwards. We diversify our selective search\n",
    "(1) by using a variety of colour spaces with different invariance\n",
    "properties, (2) by using different similarity measures si j, and (3)\n",
    "by varying our starting regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">4 Use Selective search for region proposals</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, In selective search, we start with many tiny initial regions. We use a greedy algorithm to grow a region. First we locate two most similar regions and merge them together. Similarity S between region a and b is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(a,b)=S_{texture}(a,b)+S_{size}(a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $S_{texture}(a,b)$ measures the ***visual similarity***, and $S_{size}$ prefers ***merging smaller regions together*** to avoid a single region from gobbling up all others one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue merging regions until everything is combined together. In the first row, we show how we grow the regions, and the blue rectangles in the second rows show all possible region proposals we made during the merging. The green rectangle are the target objects that we want to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Selective Search to R-CNN(Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every region proposal, we use a CNN to extract the features. Since a CNN takes a fixed-size image, we wrap a proposed region into a 227 x 227 RGB images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/5.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features with a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will then process by a CNN to extract a 4096-dimensional feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/6.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply a SVM classifier to identify the object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/7.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding box regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original boundary box proposal may need further refinement. We apply a regressor to calculate the final red box from the initial blue proposal region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/9.png'>\n",
    "\n",
    "<img src='asset/7_2/10.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the R-CNN classifies objects in a picture and produces the corresponding boundary box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/7_2/11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
