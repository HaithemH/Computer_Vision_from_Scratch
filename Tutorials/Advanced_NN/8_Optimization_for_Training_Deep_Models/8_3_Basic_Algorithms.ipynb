{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.3 Basic Algorithms</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have previously introduced the gradient descent algorithm that\n",
    "follows the gradient of an entire training set downhill. This may be accelerated\n",
    "considerably by using stochastic gradient descent to follow the gradient of randomly\n",
    "selected minibatches downhilk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.3.1 Stochastic Gradient Descent</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) and its variants are probably the most used\n",
    "optimization algorithms for machine learning in general and for deep learning\n",
    "in particular. As discussed in section 8.1.3, it is possible to obtain an unbiased\n",
    "estimate of the gradient by taking the average gradient on a minibatch of m\n",
    "examples drawn i.i.d from the data generating distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 8.1 shows how to follow this estimate of the gradient downhill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial parameter for the SGD algorithm is the learning rate. Previously, we\n",
    "have described SGD as using a fixed learning rate $\\epsilon$. In practice, it is necessary to\n",
    "gradually decrease the learning rate over time, so we now denote the learning rate\n",
    "at iteration k as $\\epsilon_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the SGD gradient estimator introduces a source of noise (the\n",
    "random sampling of m training examples) that does not vanish even when we arrive\n",
    "at a minimum. By comparison, the true gradient of the total cost function becomes\n",
    "small and then 0 when we approach and reach a minimum using batch gradient\n",
    "descent, so batch gradient descent can use a fixed learning rate. A sufficient\n",
    "condition to guarantee convergence of SGD is that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is common to decay the learning rate linearly until iteration τ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with α = k/τ. After iteration τ , it is common to leave $\\epsilon$ constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.3.2 Momentum</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While stochastic gradient descent remains a very popular optimization strategy,\n",
    "learning with it can sometimes be slow. The method of momentum (Polyak, 1964)\n",
    "is designed to accelerate learning, especially in the face of high curvature, small but\n",
    "consistent gradients, or noisy gradients. The momentum algorithm accumulates\n",
    "an exponentially decaying moving average of past gradients and continues to move\n",
    "in their direction. The effect of momentum is illustrated in figure 8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, the momentum algorithm introduces a variable v that plays the role\n",
    "of velocity—it is the direction and speed at which the parameters move through\n",
    "parameter space. The velocity is set to an exponentially decaying average of the\n",
    "negative gradient. The name momentum derives from a physical analogy, in\n",
    "which the negative gradient is a force moving a particle through parameter space,\n",
    "according to Newton’s laws of motion. Momentum in physics is mass times velocity.\n",
    "In the momentum learning algorithm, we assume unit mass, so the velocity vectorv\n",
    "may also be regarded as the momentum of the particle. A hyperparameter α ∈ [0,1)\n",
    "determines how quickly the contributions of previous gradients exponentially decay.\n",
    "The update rule is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger α is relative to $\\epsilon$, the more previous gradients affect the current direction.\n",
    "The SGD algorithm with momentum is given in algorithm 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 8.5: Momentum aims primarily to solve two problems: poor conditioning of the\n",
    "Hessian matrix and variance in the stochastic gradient. Here, we illustrate how momentum\n",
    "overcomes the first of these two problems. The contour lines depict a quadratic loss\n",
    "function with a poorly conditioned Hessian matrix. The red path cutting across the\n",
    "contours indicates the path followed by the momentum learning rule as it minimizes this\n",
    "function. At each step along the way, we draw an arrow indicating the step that gradient\n",
    "descent would take at that point. We can see that a poorly conditioned quadratic objective\n",
    "looks like a long, narrow valley or canyon with steep sides. Momentum correctly traverses\n",
    "the canyon lengthwise, while gradient steps waste time moving back and forth across the\n",
    "narrow axis of the canyon. Compare also figure 4.6, which shows the behavior of gradient\n",
    "descent without momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_3/7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
