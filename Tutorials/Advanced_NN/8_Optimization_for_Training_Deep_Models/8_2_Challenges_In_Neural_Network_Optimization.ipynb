{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.2 Challenges in Neural Network Optimization</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization in general is an extremely difficult task. Traditionally, machine\n",
    "learning has avoided the difficulty of general optimization by carefully designing\n",
    "the objective function and constraints to ensure that the optimization problem is\n",
    "convex. When training neural networks, we must confront the general non-convex\n",
    "case. Even convex optimization is not without its complications. In this section,\n",
    "we summarize several of the most prominent challenges involved in optimization\n",
    "for training deep models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.2.1 ill-Conditioning</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some challenges arise even when optimizing convex functions. Of these, the most\n",
    "prominent is ill-conditioning of the Hessian matrix H. This is a very general\n",
    "problem in most numerical optimization, convex or otherwise. \n",
    "\n",
    "The ill-conditioning problem is generally believed to be present in neural\n",
    "network training problems. ill-conditioning can manifest by causing SGD to get\n",
    "“stuck” in the sense that even very small steps increase the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a second-order Taylor series expansion of the\n",
    "cost function predicts that a gradient descent step of $−\\epsilon g$ will add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_2/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to the cost. Ill-conditioning of the gradient becomes a problem when $1/2*\\epsilon H g - \\epsilon g^{T} g$\n",
    "exceeds $\\epsilon g^{T}g$. To determine whether ill-conditioning is detrimental to a neural\n",
    "network training task, one can monitor the squared gradient norm $g^{T}g$ and  the $g^{T}Hg$ term. In many cases, the gradient norm does not shrink significantly\n",
    "throughout learning, but the $g^{T}Hg$ term grows by more than an order of magnitude.\n",
    "The result is that learning becomes very slow despite the presence of a strong\n",
    "gradient because the learning rate must be shrunk to compensate for even stronger\n",
    "curvature. Figure 8.1 shows an example of the gradient increasing significantly\n",
    "during the successful training of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_2/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 8.1: Gradient descent often does not arrive at a critical point of any kind. In this\n",
    "example, the gradient norm increases throughout training of a convolutional network used\n",
    "for object detection. (Left)A scatterplot showing how the norms of individual gradient\n",
    "evaluations are distributed over time. To improve legibility, only one gradient norm\n",
    "is plotted per epoch. The running average of all gradient norms is plotted as a solid\n",
    "curve. The gradient norm clearly increases over time, rather than decreasing as we would\n",
    "expect if the training process converged to a critical point. (Right)Despite the increasing\n",
    "gradient, the training process is reasonably successful. The validation set classification\n",
    "error decreases to a low level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though ill-conditioning is present in other settings besides neural network\n",
    "training, some of the techniques used to combat it in other contexts are less\n",
    "applicable to neural networks. For example, Newton’s method is an excellent tool\n",
    "for minimizing convex functions with poorly conditioned Hessian matrices, but in\n",
    "the subsequent sections we will argue that Newton’s method requires significant\n",
    "modification before it can be applied to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.2.2 Local Minima</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most prominent features of a convex optimization problem is that it\n",
    "can be reduced to the problem of finding a local minimum. Any local minimum is guaranteed to be a global minimum. Some convex functions have a flat region at\n",
    "the bottom rather than a single global minimum point, but any point within such\n",
    "a flat region is an acceptable solution. When optimizing a convex function, we\n",
    "know that we have reached a good solution if we find a critical point of any kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***With non-convex functions, such as neural nets***, it is possible to have many\n",
    "local minima. Indeed, nearly any deep model is essentially guaranteed to have\n",
    "an extremely large number of local minima. However, as we will see, this is not\n",
    "necessarily a major problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks and any models with multiple equivalently parametrized latent\n",
    "variables all have multiple local minima because of the model ***identifiability\n",
    "problem***. A model is said to be identifiable if a sufficiently large training set can\n",
    "rule out all but one setting of the model’s parameters. Models with latent variables\n",
    "are often not identifiable because we can obtain equivalent models by exchanging\n",
    "latent variables with each other. For example, we could take a neural network and\n",
    "modify layer 1 by swapping the incoming weight vector for unit i with the incoming\n",
    "weight vector for unit j, then doing the same for the outgoing weight vectors. If we\n",
    "have m layers with n units each, then there are n!m ways of arranging the hidden\n",
    "units. This kind of non-identifiability is known as ***weight space symmetry***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to weight space symmetry, many kinds of neural networks have\n",
    "additional causes of non-identifiability. For example, in any rectified linear or\n",
    "maxout network, we can scale all of the incoming weights and biases of a unit by\n",
    "α if we also scale all of its outgoing weights by 1/α. This means that—if the cost\n",
    "function does not include terms such as weight decay that depend directly on the\n",
    "weights rather than the models’ outputs—every local minimum of a rectified linear\n",
    "or maxout network lies on an (m × n)-dimensional hyperbola of equivalent local\n",
    "minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These model identifiability issues mean that there can be an extremely large\n",
    "or even uncountably infinite amount of local minima in a neural network cost\n",
    "function. However, all of these local minima arising from non-identifiability are\n",
    "equivalent to each other in cost function value. As a result, these local minima are\n",
    "not a problematic form of non-convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minima can be problematic if they have high cost in comparison to the\n",
    "global minimum. One can construct small neural networks, even without hidden\n",
    "units, that have local minima with higher cost than the global minimum (Sontag\n",
    "and Sussman, 1989; Brady et al., 1989; Gori and Tesi, 1992). If local minima\n",
    "with high cost are common, this could pose a serious problem for gradient-based\n",
    "optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains an open question whether there are many local minima of high cost for networks of practical interest and whether optimization algorithms encounter\n",
    "them. For many years, most practitioners believed that local minima were a\n",
    "common problem plaguing neural network optimization. Today, that does not\n",
    "appear to be the case. The problem remains an active area of research, but experts\n",
    "now suspect that, for sufficiently large neural networks, most local minima have a\n",
    "low cost function value, and that it is not important to find a true global minimum\n",
    "rather than to find a point in parameter space that has low but not minimal cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many practitioners attribute nearly all difficulty with neural network optimization to local minima. We encourage practitioners to carefully test for specific\n",
    "problems. A test that can rule out local minima as the problem is to plot the\n",
    "norm of the gradient over time. If the norm of the gradient does not shrink to\n",
    "insignificant size, the problem is neither local minima nor any other kind of critical\n",
    "point. This kind of negative test can rule out local minima. In high dimensional\n",
    "spaces, it can be very difficult to positively establish that local minima are the\n",
    "problem. Many structures other than local minima also have small gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.2.3 Plateaus, Saddle Points and Other Flat Regions</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many high-dimensional non-convex functions, local minima (and maxima)\n",
    "are in fact rare compared to another kind of point with zero gradient: a saddle\n",
    "point. Some points around a saddle point have greater cost than the saddle point,\n",
    "while others have a lower cost. At a saddle point, the Hessian matrix has both\n",
    "positive and negative eigenvalues. Points lying along eigenvectors associated with\n",
    "positive eigenvalues have greater cost than the saddle point, while points lying\n",
    "along negative eigenvalues have lower value. We can think of a saddle point as\n",
    "being a local minimum along one cross-section of the cost function and a local\n",
    "maximum along another cross-section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classes of random functions exhibit the following behavior: in lowdimensional spaces, local minima are common. In higher dimensional spaces, local\n",
    "minima are rare and saddle points are more common. For a function f : Rn → R of\n",
    "this type, the expected ratio of the number of saddle points to local minima grows\n",
    "exponentially with n. To understand the intuition behind this behavior, observe\n",
    "that the Hessian matrix at a local minimum has only positive eigenvalues. The\n",
    "Hessian matrix at a saddle point has a mixture of positive and negative eigenvalues.\n",
    "Imagine that the sign of each eigenvalue is generated by flipping a coin. In a single\n",
    "dimension, it is easy to obtain a local minimum by tossing a coin and getting heads\n",
    "once. In n-dimensional space, it is exponentially unlikely that all n coin tosses will be heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An amazing property of many random functions is that the eigenvalues of the\n",
    "Hessian become more likely to be positive as we reach regions of lower cost. In\n",
    "our coin tossing analogy, this means we are more likely to have our coin come up\n",
    "heads n times if we are at a critical point with low cost. This means that local\n",
    "minima are much more likely to have low cost than high cost. Critical points with\n",
    "high cost are far more likely to be saddle points. Critical points with extremely\n",
    "high cost are more likely to be local maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens for many classes of random functions. Does it happen for neural\n",
    "networks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders\n",
    "(feedforward networks trained to copy their input to their output, described in\n",
    "chapter 14) with no nonlinearities have global minima and saddle points but no\n",
    "local minima with higher cost than the global minimum. They observed without\n",
    "proof that these results extend to deeper networks without nonlinearities. The\n",
    "output of such networks is a linear function of their input, but they are useful\n",
    "to study as a model of nonlinear neural networks because their loss function is\n",
    "a non-convex function of their parameters. Such networks are essentially just\n",
    "multiple matrices composed together. Saxe et al. (2013) provided exact solutions\n",
    "to the complete learning dynamics in such networks and showed that learning in\n",
    "these models captures many of the qualitative features observed in the training of\n",
    "deep models with nonlinear activation functions. Dauphin et al. (2014) showed\n",
    "experimentally that real neural networks also have loss functions that contain very\n",
    "many high-cost saddle points. Choromanska et al. (2014) provided additional\n",
    "theoretical arguments, showing that another class of high-dimensional random\n",
    "functions related to neural networks does so as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the implications of the proliferation of saddle points for training algorithms? For first-order optimization algorithms that use only gradient information,\n",
    "the situation is unclear. The gradient can often become very small near a saddle\n",
    "point. On the other hand, gradient descent empirically seems to be able to escape\n",
    "saddle points in many cases. Goodfellow et al. (2015) provided visualizations of\n",
    "several learning trajectories of state-of-the-art neural networks, with an example\n",
    "given in figure 8.2. These visualizations show a flattening of the cost function near\n",
    "a prominent saddle point where the weights are all zero, but they also show the\n",
    "gradient descent trajectory rapidly escaping this region. Goodfellow et al. (2015)\n",
    "also argue that continuous-time gradient descent may be shown analytically to be\n",
    "repelled from, rather than attracted to, a nearby saddle point, but the situation\n",
    "may be different for more realistic uses of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Newton’s method, it is clear that saddle points constitute a problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_2/3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 8.2: A visualization of the cost function of a neural network. Image adapted\n",
    "with permission from Goodfellow et al. (2015). These visualizations appear similar for\n",
    "feedforward neural networks, convolutional networks, and recurrent networks applied\n",
    "to real object recognition and natural language processing tasks. Surprisingly, these\n",
    "visualizations usually do not show many conspicuous obstacles. Prior to the success of\n",
    "stochastic gradient descent for training very large models beginning in roughly 2012,\n",
    "neural net cost function surfaces were generally believed to have much more non-convex\n",
    "structure than is revealed by these projections. The primary obstacle revealed by this\n",
    "projection is a saddle point of high cost near where the parameters are initialized, but, as\n",
    "indicated by the blue path, the SGD training trajectory escapes this saddle point readily.\n",
    "Most of training time is spent traversing the relatively flat valley of the cost function,\n",
    "which may be due to high noise in the gradient, poor conditioning of the Hessian matrix\n",
    "in this region, or simply the need to circumnavigate the tall “mountain” visible in the\n",
    "figure via an indirect arcing path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is designed to move “downhill” and is not explicitly designed\n",
    "to seek a critical point. Newton’s method, however, is designed to solve for a\n",
    "point where the gradient is zero. Without appropriate modification, it can jump\n",
    "to a saddle point. The proliferation of saddle points in high dimensional spaces\n",
    "presumably explains why second-order methods have not succeeded in replacing\n",
    "gradient descent for neural network training. Dauphin et al. (2014) introduced a\n",
    "saddle-free Newton method for second-order optimization and showed that it\n",
    "improves significantly over the traditional version. Second-order methods remain\n",
    "difficult to scale to large neural networks, but this saddle-free approach holds\n",
    "promise if it could be scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other kinds of points with zero gradient besides minima and saddle\n",
    "points. There are also maxima, which are much like saddle points from the\n",
    "perspective of optimization—many algorithms are not attracted to them, but\n",
    "unmodified Newton’s method is. Maxima of many classes of random functions\n",
    "become exponentially rare in high dimensional space, just like minima do.\n",
    "\n",
    "There may also be wide, flat regions of constant value. In these locations, the\n",
    "gradient and also the Hessian are all zero. Such degenerate locations pose major\n",
    "problems for all numerical optimization algorithms. In a convex problem, a wide,\n",
    "flat region must consist entirely of global minima, but in a general optimization\n",
    "problem, such a region could correspond to a high value of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.2.4 Cliffs and Exploding Gradients</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks with many layers often have extremely steep regions resembling\n",
    "cliffs, as illustrated in figure 8.3. These result from the multiplication of several\n",
    "large weights together. On the face of an extremely steep cliff structure, the\n",
    "gradient update step can move the parameters extremely far, usually jumping off\n",
    "of the cliff structure altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_2/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 8.3: The objective function for highly nonlinear deep neural networks or for\n",
    "recurrent neural networks often contains sharp nonlinearities in parameter space resulting\n",
    "from the multiplication of several parameters. These nonlinearities give rise to very\n",
    "high derivatives in some places. When the parameters get close to such a cliff region, a\n",
    "gradient descent update can catapult the parameters very far, possibly losing most of the\n",
    "optimization work that had been done. Figure adapted with permission from Pascanu\n",
    "et al. (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.2.5 Long-Term Dependencies</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another difficulty that neural network optimization algorithms must overcome\n",
    "arises when the computational graph becomes extremely deep. Feedforward\n",
    "networks with many layers have such deep computational graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose that a computational graph contains a path that consists\n",
    "of repeatedly multiplying by a matrix W. After t steps, this is equivalent to multiplying by $W^t$ . Suppose that W has an eigendecomposition W = V diag(λ)V −1.\n",
    "In this simple case, it is straightforward to see that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_2/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any eigenvalues $λ_{i}$ that are not near an absolute value of 1 will either explode if they\n",
    "are greater than 1 in magnitude or vanish if they are less than 1 in magnitude. The\n",
    "***vanishing and exploding gradient problem*** refers to the fact that gradients\n",
    "through such a graph are also scaled according to diag(λ)t. Vanishing gradients\n",
    "make it difficult to know which direction the parameters should move to improve\n",
    "the cost function, while exploding gradients can make learning unstable. The cliff\n",
    "structures described earlier that motivate gradient clipping are an example of the\n",
    "exploding gradient phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repeated multiplication by W at each time step described here is very\n",
    "similar to the power method algorithm used to find the largest eigenvalue of\n",
    "a matrix W and the corresponding eigenvector. From this point of view it is\n",
    "not surprising that $x^{T}W_{t}$ will eventually discard all components of x that are\n",
    "orthogonal to the principal eigenvector of W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent networks use the same matrix W at each time step, but feedforward\n",
    "networks do not, so even very deep feedforward networks can largely avoid the\n",
    "vanishing and exploding gradient problem (Sussillo, 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
