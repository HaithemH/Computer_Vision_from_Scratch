{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.5 Algorithms with Adaptive Learning Rates</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network researchers have long realized that the learning rate was reliably one\n",
    "of the hyperparameters that is the most difficult to set because it has a significant\n",
    "impact on model performance. The\n",
    "cost is often highly sensitive to some directions in parameter space and insensitive\n",
    "to others. The momentum algorithm can mitigate these issues somewhat, but\n",
    "does so at the expense of introducing another hyperparameter. In the face of this,\n",
    "it is natural to ask if there is another way. If we believe that the directions of\n",
    "sensitivity are somewhat axis-aligned, it can make sense to use a separate learning rate for each parameter, and automatically adapt these learning rates throughout\n",
    "the course of learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***delta-bar-delta*** algorithm (Jacobs, 1988) is an early heuristic approach\n",
    "to adapting individual learning rates for model parameters during training. The\n",
    "approach is based on a simple idea: if the partial derivative of the loss, with respect\n",
    "to a given model parameter, remains the same sign, then the learning rate should\n",
    "increase. If the partial derivative with respect to that parameter changes sign,\n",
    "then the learning rate should decrease. Of course, this kind of rule can only be\n",
    "applied to full batch optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More recently, a number of incremental (or mini-batch-based) methods have\n",
    "been introduced that adapt the learning rates of model parameters. This section\n",
    "will briefly review a few of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.5.1 AdaGrad</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaGrad algorithm, shown in algorithm 8.4, individually adapts the learning\n",
    "rates of all model parameters by scaling them inversely proportional to the square\n",
    "root of the sum of all of their historical squared values (Duchi et al., 2011). The\n",
    "parameters with the largest partial derivative of the loss have a correspondingly\n",
    "rapid decrease in their learning rate, while parameters with small partial derivatives\n",
    "have a relatively small decrease in their learning rate. The net effect is greater\n",
    "progress in the more gently sloped directions of parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_5/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of convex optimization, the AdaGrad algorithm enjoys some\n",
    "desirable theoretical properties. However, empirically it has been found that—for\n",
    "training deep neural network models—the accumulation of squared gradients from\n",
    "the beginning of training can result in a premature and excessive decrease in the\n",
    "effective learning rate. AdaGrad performs well for some but not all deep learning\n",
    "models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.5.2 RMSProp</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSProp algorithm (Hinton, 2012) modifies AdaGrad to perform better in\n",
    "the non-convex setting by changing the gradient accumulation into an exponentially\n",
    "weighted moving average. AdaGrad is designed to converge rapidly when applied\n",
    "to a convex function. When applied to a non-convex function to train a neural\n",
    "network, the learning trajectory may pass through many different structures and\n",
    "eventually arrive at a region that is a locally convex bowl. AdaGrad shrinks the\n",
    "learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure.\n",
    "RMSProp uses an exponentially decaying average to discard history from the\n",
    "extreme past so that it can converge rapidly after finding a convex bowl, as if it\n",
    "were an instance of the AdaGrad algorithm initialized within that bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp is shown in its standard form in algorithm 8.5 and combined with\n",
    "Nesterov momentum in algorithm 8.6. Compared to AdaGrad, the use of the\n",
    "moving average introduces a new hyperparameter, ρ, that controls the length scale\n",
    "of the moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_5/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirically, RMSProp has been shown to be an effective and practical optimization algorithm for deep neural networks. It is currently one of the go-to\n",
    "optimization methods being employed routinely by deep learning practitioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_5/3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.5.3 Adam</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam (Kingma and Ba, 2014) is yet another adaptive learning rate optimization\n",
    "algorithm and is presented in algorithm 8.7. The name “Adam” derives from\n",
    "the phrase “adaptive moments.” In the context of the earlier algorithms, it is\n",
    "perhaps best seen as a variant on the combination of RMSProp and momentum\n",
    "with a few important distinctions. First, in Adam, momentum is incorporated\n",
    "directly as an estimate of the first order moment (with exponential weighting) of\n",
    "the gradient. The most straightforward way to add momentum to RMSProp is to\n",
    "apply momentum to the rescaled gradients. The use of momentum in combination\n",
    "with rescaling does not have a clear theoretical motivation. Second, Adam includes bias corrections to the estimates of both the first-order moments (the momentum\n",
    "term) and the (uncentered) second-order moments to account for their initialization\n",
    "at the origin (see algorithm 8.7). RMSProp also incorporates an estimate of the\n",
    "(uncentered) second-order moment, however it lacks the correction factor. Thus,\n",
    "unlike in Adam, the RMSProp second-order moment estimate may have high bias\n",
    "early in training. Adam is generally regarded as being fairly robust to the choice\n",
    "of hyperparameters, though the learning rate sometimes needs to be changed from\n",
    "the suggested default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_5/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.5.4 Choosing the Right Optimization Algorithm</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we discussed a series of related algorithms that each seek to address\n",
    "the challenge of optimizing deep models by adapting the learning rate for each\n",
    "model parameter. At this point, a natural question is: which algorithm should one\n",
    "choose?\n",
    "\n",
    "Unfortunately, there is currently no consensus on this point. Schaul et al. (2014)\n",
    "presented a valuable comparison of a large number of optimization algorithms\n",
    "across a wide range of learning tasks. While the results suggest that the family of\n",
    "algorithms with adaptive learning rates (represented by RMSProp and AdaDelta)\n",
    "performed fairly robustly, no single best algorithm has emerged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the most popular optimization algorithms actively in use include\n",
    "SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta\n",
    "and Adam. The choice of which algorithm to use, at this point, seems to depend largely on the user’s familiarity with the algorithm (for ease of hyperparameter\n",
    "tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
