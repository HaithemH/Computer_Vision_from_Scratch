{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Optimization for Training Deep Models</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning algorithms involve optimization in many contexts. For example,\n",
    "performing inference in models such as PCA involves solving an optimization\n",
    "problem. We often use analytical optimization to write proofs or design algorithms.\n",
    "Of all of the many optimization problems involved in deep learning, the most\n",
    "difficult is neural network training. It is quite common to invest days to months of\n",
    "time on hundreds of machines in order to solve even a single instance of the neural\n",
    "network training problem. Because this problem is so important and so expensive,\n",
    "a specialized set of optimization techniques have been developed for solving it.\n",
    "This chapter presents these optimization techniques for neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8-th chapter focuses on one particular case of optimization: finding the parameters θ of a neural network that significantly reduce a cost function J(θ), which\n",
    "typically includes a performance measure evaluated on the entire training set as\n",
    "well as additional regularization terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a description of how optimization used as a training algorithm\n",
    "for a machine learning task differs from pure optimization. Next, we present several\n",
    "of the concrete challenges that make optimization of neural networks difficult. We\n",
    "then define several practical algorithms, including both optimization algorithms\n",
    "themselves and strategies for initializing the parameters. More advanced algorithms\n",
    "adapt their learning rates during training or leverage information contained in the second derivatives of the cost function. Finally, we conclude with a review of\n",
    "several optimization strategies that are formed by combining simple optimization\n",
    "algorithms into higher-level procedures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
