{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">8.7 Optimization Strategies and Meta-Algorithms</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many optimization techniques are not exactly algorithms, but rather general\n",
    "templates that can be specialized to yield algorithms, or subroutines that can be\n",
    "incorporated into many different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">8.7.1 Batch Normalization</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization (Ioffe and Szegedy, 2015) is one of the most exciting recent\n",
    "innovations in optimizing deep neural networks and it is actually not an optimization\n",
    "algorithm at all. Instead, it is a method of adaptive reparametrization, motivated\n",
    "by the difficulty of training very deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very deep models involve the composition of several functions or layers. The\n",
    "gradient tells how to update each parameter, under the assumption that the other\n",
    "layers do not change. In practice, we update all of the layers simultaneously.\n",
    "When we make the update, unexpected results can happen because many functions\n",
    "composed together are changed simultaneously, using updates that were computed\n",
    "under the assumption that the other functions remain constant. \n",
    "\n",
    "As a simple example, suppose we have a deep neural network that has only one unit per layer\n",
    "and does not use an activation function at each hidden layer: $yˆ = xw_{1}w_{2}w_{3} . . . w_{l}$.\n",
    "Here, wi provides the weight used by layer i. The output of layer i is $h_{i} = h_{i−1}w_{i}$.\n",
    "The output $yˆ$ is a linear function of the input x, but a nonlinear function of the\n",
    "weights $w_{i}$. Suppose our cost function has put a gradient of 1 on $yˆ$, so we wish to\n",
    "decrease $yˆ$ slightly. The back-propagation algorithm can then compute a gradient\n",
    "$g = ∇wyˆ$. Consider what happens when we make an update $w ← w − \\epsilon g$. The\n",
    "first-order Taylor series approximation of $yˆ$ predicts that the value of $yˆ$ will decrease\n",
    "by $\\epsilon g^{T} g$. If we wanted to decrease $yˆ$ by 0.1, this first-order information available in\n",
    "the gradient suggests we could set the learning rate $\\epsilon$ to 0.1/ $g^{T}g$ . However, the actual\n",
    "update will include second-order and third-order effects, on up to effects of order l.\n",
    "The new value of $yˆ$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_7/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of one second-order term arising from this update is $\\epsilon^{2}g_{1} g_{2} \\Pi_{i=3}^l w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This term might be negligible if $\\Pi_{i=3}^l w_i$ is small, or might be exponentially large\n",
    "if the weights on layers 3 through l are greater than 1. This makes it very hard\n",
    "to choose an appropriate learning rate, because the effects of an update to the\n",
    "parameters for one layer depends so strongly on all of the other layers. Second-order\n",
    "optimization algorithms address this issue by computing an update that takes these\n",
    "second-order interactions into account, but we can see that in very deep networks,\n",
    "even higher-order interactions can be significant. Even second-order optimization\n",
    "algorithms are expensive and usually require numerous approximations that prevent\n",
    "them from truly accounting for all significant second-order interactions. Building\n",
    "an n-th order optimization algorithm for n > 2 thus seems hopeless. What can we\n",
    "do instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization provides an elegant way of reparametrizing almost any deep\n",
    "network. The reparametrization significantly reduces the problem of coordinating\n",
    "updates across many layers. Batch normalization can be applied to any input\n",
    "or hidden layer in a network. Let H be a minibatch of activations of the layer\n",
    "to normalize, arranged as a design matrix, with the activations for each example\n",
    "appearing in a row of the matrix. To normalize H, we replace it with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_7/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where µ is a vector containing the mean of each unit and σ is a vector containing\n",
    "the standard deviation of each unit. The arithmetic here is based on broadcasting\n",
    "the vector µ and the vector σ to be applied to every row of the matrix H . Within\n",
    "each row, the arithmetic is element-wise, so $H_{i,j}$ is normalized by subtracting $µ_{j}$ and dividing by $σ_j$. The rest of the network then operates on $H^{'}$ in exactly the\n",
    "same way that the original network operated on H."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At training time,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_7/3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_7/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where δ is a small positive value such as $10^{−8}$ imposed to avoid encountering\n",
    "the undefined gradient of √z at z = 0. Crucially, we back-propagate through\n",
    "these operations for computing the mean and the standard deviation, and for\n",
    "applying them to normalize H. This means that the gradient will never propose\n",
    "an operation that acts simply to increase the standard deviation or mean of\n",
    "$h_i$; the normalization operations remove the effect of such an action and zero\n",
    "out its component in the gradient. This was a major innovation of the batch\n",
    "normalization approach. Previous approaches had involved adding penalties to\n",
    "the cost function to encourage units to have normalized activation statistics or\n",
    "involved intervening to renormalize unit statistics after each gradient descent step.\n",
    "The former approach usually resulted in imperfect normalization and the latter\n",
    "usually resulted in significant wasted time as the learning algorithm repeatedly\n",
    "proposed changing the mean and variance and the normalization step repeatedly\n",
    "undid this change. Batch normalization reparametrizes the model to make some\n",
    "units always be standardized by definition, deftly sidestepping both problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At test time, µ and σ may be replaced by running averages that were collected\n",
    "during training time. This allows the model to be evaluated on a single example,\n",
    "without needing to use definitions of µ and σ that depend on an entire minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the yˆ = $xw_{1}w_{2} . . . w_{l}$ example, we see that we can mostly resolve the\n",
    "difficulties in learning this model by normalizing $h_{l−1}$. Suppose that x is drawn\n",
    "from a unit Gaussian. Then $h_{l−1}$ will also come from a Gaussian, because the\n",
    "transformation from x to $h_{l}$ is linear. However, $h_{l−1}$ will no longer have zero mean\n",
    "and unit variance. After applying batch normalization, we obtain the normalized\n",
    "$\\hat{h}_{l−1}$ that restores the zero mean and unit variance properties. For almost any\n",
    "update to the lower layers, $\\hat{h}_{l−1}$ will remain a unit Gaussian. The output yˆ may\n",
    "then be learned as a simple linear function yˆ = $w_{l} \\hat{h}_{l−1}$. Learning in this model is\n",
    "now very simple because the parameters at the lower layers simply do not have an\n",
    "effect in most cases; their output is always renormalized to a unit Gaussian. In\n",
    "some corner cases, the lower layers can have an effect. Changing one of the lower\n",
    "layer weights to 0 can make the output become degenerate, and changing the sign of one of the lower weights can flip the relationship between $\\hat{h}_{l−1}$ and y. These\n",
    "situations are very rare. Without normalization, nearly every update would have\n",
    "an extreme effect on the statistics of $h_{l−1}$. Batch normalization has thus made\n",
    "this model significantly easier to learn. In this example, the ease of learning of\n",
    "course came at the cost of making the lower layers useless. In our linear example,\n",
    "the lower layers no longer have any harmful effect, but they also no longer have\n",
    "any beneficial effect. This is because we have normalized out the first and second\n",
    "order statistics, which is all that a linear network can influence. In a deep neural\n",
    "network with nonlinear activation functions, the lower layers can perform nonlinear\n",
    "transformations of the data, so they remain useful. Batch normalization acts to\n",
    "standardize only the mean and variance of each unit in order to stabilize learning,\n",
    "but allows the relationships between units and the nonlinear statistics of a single\n",
    "unit to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the final layer of the network is able to learn a linear transformation,\n",
    "we may actually wish to remove all linear relationships between units within a\n",
    "layer. Indeed, this is the approach taken by Desjardins et al. (2015), who provided\n",
    "the inspiration for batch normalization. Unfortunately, eliminating all linear\n",
    "interactions is much more expensive than standardizing the mean and standard\n",
    "deviation of each individual unit, and so far batch normalization remains the most\n",
    "practical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the mean and standard deviation of a unit can reduce the expressive\n",
    "power of the neural network containing that unit. In order to maintain the\n",
    "expressive power of the network, it is common to replace the batch of hidden unit\n",
    "activations H with γH' +β rather than simply the normalized H'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables\n",
    "γ and β are learned parameters that allow the new variable to have any mean\n",
    "and standard deviation. At first glance, this may seem useless—why did we set\n",
    "the mean to 0, and then introduce a parameter that allows it to be set back to\n",
    "any arbitrary value β? The answer is that the new parametrization can represent\n",
    "the same family of functions of the input as the old parametrization, but the new\n",
    "parametrization has different learning dynamics. In the old parametrization, the\n",
    "mean of H was determined by a complicated interaction between the parameters\n",
    "in the layers below H. In the new parametrization, the mean of γH' + β is\n",
    "determined solely by β. The new parametrization is much easier to learn with\n",
    "gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most neural network layers take the form of φ(XW + b) where φ is some\n",
    "fixed nonlinear activation function such as the rectified linear transformation. It\n",
    "is natural to wonder whether we should apply batch normalization to the input\n",
    "X , or to the transformed value XW + b. Ioffe and Szegedy (2015) recommend the latter. More specifically, XW + b should be replaced by a normalized version\n",
    "of XW. The bias term should be omitted because it becomes redundant with\n",
    "the β parameter applied by the batch normalization reparametrization. The input\n",
    "to a layer is usually the output of a nonlinear activation function such as the\n",
    "rectified linear function in a previous layer. The statistics of the input are thus\n",
    "more non-Gaussian and less amenable to standardization by linear operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convolutional networks, it is important to apply the\n",
    "same normalizing µ and σ at every spatial location within a feature map, so that\n",
    "the statistics of the feature map remain the same regardless of spatial location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">8.7.1.1 Batch Normalization Layers</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch normalization methods for fully-connected layers and convolutional layers are slightly different. This is due to the dimensionality of the data generated by convolutional layers. We discuss both cases below. Note that one of the key differences between BN and other layers is that BN operates on a a full minibatch at a time (otherwise it cannot compute the mean and variance parameters per batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div align=\"center\">8.7.1.1.1 Fully-Connected Layers</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we apply the batch normalization layer between the affine transformation and the activation function in a fully-connected layer. In the following, we denote by  u  the input and by  x=Wu+b  the output of the linear transform. This yields the following variant of BN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/8_7/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that mean and variance are computed on the same minibatch  B  on which the transformation is applied. Also recall that the scaling coefficient  γ  and the offset  β  are parameters that need to be learned. They ensure that the effect of batch normalization can be neutralized as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div align=\"center\">8.7.1.1.2 Convolutional Layers</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convolutional layers, batch normalization occurs after the convolution computation and before the application of the activation function. If the convolution computation outputs multiple channels, we need to carry out batch normalization for each of the outputs of these channels, and each channel has an independent scale parameter and shift parameter, both of which are scalars. Assume that there are  m  examples in the mini-batch. On a single channel, we assume that the height and width of the convolution computation output are  p  and  q , respectively. We need to carry out batch normalization for  m×p×q  elements in this channel simultaneously. While carrying out the standardization computation for these elements, we use the same mean and variance. In other words, we use the means and variances of the  m×p×q  elements in this channel rather than one per pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div align=\"center\">8.7.1.1.3 Batch Normalization During Prediction</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At prediction time, we might not have the luxury of computing offsets per batch—we might be required to make one prediction at a time. Secondly, the uncertainty in  μ  and  σ , as arising from a minibatch are undesirable once we’ve trained the model. One way to mitigate this is to compute more stable estimates on a larger set for once (e.g. via a moving average) and then fix them at prediction time. Consequently, BN behaves differently during training and at test time (recall that dropout also behaves differently at train and test times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">8.7.1.2 Implementation from Scratch</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import d2l\n",
    "# from mxnet import autograd, gluon, nd, init\n",
    "# from mxnet.gluon import nn\n",
    "\n",
    "# def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "#     # Use autograd to determine whether the current mode is training mode or\n",
    "#     # prediction mode\n",
    "#     if not autograd.is_training():\n",
    "#         # If it is the prediction mode, directly use the mean and variance\n",
    "#         # obtained from the incoming moving average\n",
    "#         X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)\n",
    "#     else:\n",
    "#         assert len(X.shape) in (2, 4)\n",
    "#         if len(X.shape) == 2:\n",
    "#             # When using a fully connected layer, calculate the mean and\n",
    "#             # variance on the feature dimension\n",
    "#             mean = X.mean(axis=0)\n",
    "#             var = ((X - mean) ** 2).mean(axis=0)\n",
    "#         else:\n",
    "#             # When using a two-dimensional convolutional layer, calculate the\n",
    "#             # mean and variance on the channel dimension (axis=1). Here we\n",
    "#             # need to maintain the shape of X, so that the broadcast operation\n",
    "#             # can be carried out later\n",
    "#             mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "#             var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "#         # In training mode, the current mean and variance are used for the\n",
    "#         # standardization\n",
    "#         X_hat = (X - mean) / nd.sqrt(var + eps)\n",
    "#         # Update the mean and variance of the moving average\n",
    "#         moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "#         moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "#     Y = gamma * X_hat + beta  # Scale and shift\n",
    "#     return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can customize a BatchNorm layer. This retains the scale parameter gamma and the shift parameter beta involved in gradient finding and iteration, and it also maintains the mean and variance obtained from the moving average, so that they can be used during model prediction. The num_features parameter required by the BatchNorm instance is the number of outputs for a fully-connected layer and the number of output channels for a convolutional layer. The num_dims parameter also required by this instance is 2 for a fully-connected layer and 4 for a convolutional layer.\n",
    "\n",
    "Besides the algorithm per se, also note the design pattern in implementing layers. Typically one defines the math in a separate function, say batch_norm. This is then integrated into a custom layer that mostly focuses on bookkeeping, such as moving data to the right device context, ensuring that variables are properly initialized, keeping track of the running averages for mean and variance, etc. That way we achieve a clean separation of math and boilerplate code. Also note that for the sake of convenience we did not add automagic size inference here, hence we will need to specify the number of features throughout (the Gluon version will take care of this for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BatchNorm(nn.Block):\n",
    "#     def __init__(self, num_features, num_dims, **kwargs):\n",
    "#         super(BatchNorm, self).__init__(**kwargs)\n",
    "#         if num_dims == 2:\n",
    "#             shape = (1, num_features)\n",
    "#         else:\n",
    "#             shape = (1, num_features, 1, 1)\n",
    "#         # The scale parameter and the shift parameter involved in gradient\n",
    "#         # finding and iteration are initialized to 0 and 1 respectively\n",
    "#         self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n",
    "#         self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n",
    "#         # All the variables not involved in gradient finding and iteration are\n",
    "#         # initialized to 0 on the CPU\n",
    "#         self.moving_mean = nd.zeros(shape)\n",
    "#         self.moving_var = nd.zeros(shape)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         # If X is not on the CPU, copy moving_mean and moving_var to the\n",
    "#         # device where X is located\n",
    "#         if self.moving_mean.context != X.context:\n",
    "#             self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "#             self.moving_var = self.moving_var.copyto(X.context)\n",
    "#         # Save the updated moving_mean and moving_var\n",
    "#         Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "#             X, self.gamma.data(), self.beta.data(), self.moving_mean,\n",
    "#             self.moving_var, eps=1e-5, momentum=0.9)\n",
    "#         return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detail information about Batch Normalization look at the [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf) and [https://www.d2l.ai/chapter_convolutional-modern/batch-norm.html](https://www.d2l.ai/chapter_convolutional-modern/batch-norm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
