{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Word2vec from Scratch with NumPy</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Introduction</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I have been working with several projects related to NLP at work. Some of them had something to do with training the company’s in-house word embedding. At work, the tasks were mostly done with the help of a Python library: gensim. However, I decided to implement a Word2vec model from scratch just with the help of Python and NumPy because reinventing the wheel is usually an awesome way to learn something deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Word Embedding</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is nothing fancy but methods to represent words in a numerical way. More specifically, methods to map vocabularies to vectors.\n",
    "\n",
    "The most straightforward method could be using one-hot encoding to map each word to a one-hot vector.\n",
    "\n",
    "Although one-hot encoding is quite simple, there are several downsides. The most notable one is that it is not easy to measure relationships between words in a mathematical way.\n",
    "\n",
    "Word2vec is a neural network structure to generate word embedding by training the model on a supervised classification problem. Such a method was first introduced in the paper Efficient Estimation of Word Representations in Vector Space by Mikolov et al.,2013 and was proven to be quite successful in achieving word embedding that could used to measure syntactic and semantic similarities between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Word2vec (Skip-gram)</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Mikolov et al.,2013, two model architectures were presented, Continuous Bag-of-Words model and Skip-gram model. I will be diving into the latter in this article.\n",
    "\n",
    "In order to explain the skip-gram model, I randomly quote a piece of text from a book that I am currently reading, The Little Book of Common Sense Investing by John Bogle:\n",
    "\n",
    "***After the deduction of the costs of investing, beating the stock market is a loser’s game.***\n",
    "\n",
    "As I have mentioned above, it is a supervised classification problem that the word2vec model tries to optimize. More specifically, given a ***“context word”***, we want to train a model such that the model can predict a ***“target word”***, one of the words appeared within a predefined window size from the context word.\n",
    "\n",
    "<img src='asset/5/1.jpg'>\n",
    "\n",
    "Take the above sentence for example, given a context word “investing” and a window size of 5, we would like the model to generate one of the underlying words. (one of the words in ***deduction, of, the costs, beating, stock, market, is*** in the case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Model Overview</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the original diagram presented in the paper by Mikolov et al.,2013.\n",
    "\n",
    "<img src='asset/5/2.png'>\n",
    "\n",
    "I made another graph with a little bit more details\n",
    "\n",
    "<img src='asset/5/3.jpg'>\n",
    "\n",
    "The word embedding layer is essentially a matrix with a shape of (# of unique words in the corpus, word embedding size). Each row of the matrix represent a word in the corpus. Word embedding size is a hyper-parameter to be decided and can be thought as how many features that we would like to use to represent each word. The latter part of the model is simply a logistic regression in a neural network form.\n",
    "\n",
    "In the training process, the word embedding layer and the dense layer are being trained such that the model is able to predict target words given a context word at the end of the training process. After training such a model with a huge amount of data, the word embedding layer will end up becoming a representation of words which could demonstrate many kinds of cool relationships between words in a mathematical way. (Those who are interested in more details can refer to the original paper.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Implementation from Scratch with Python</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate training data, we tokenize text first. There are many techniques out there when it comes to tokenize text data, such as getting rid of words appearing in very high or very low frequency. I just split the text with a simple regex since the focus of the article is not tokenization.\n",
    "\n",
    "Next, we assign an integer to each word as its id. In addition, using word_to_id and id_to_word to record the mapping relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we generate training data for the model. For each context word tokens[i], generate: (tokens[i], tokens[i-window_size]), ..., (tokens[i], tokens[i-1]), (tokens[i], tokens[i+1]), ..., (tokens[i], tokens[i+window_size]). Take context word investing with a window size of 5 for example, we will generate(investing, deduction), (investing, of), (investing, the), (investing, costs), (investing, of), (investing, beating), (investing, the), (investing, stock), (investing, market), (investing, is) . Note: In the code, the training (x, y) pairs are represented in word ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow is the code for generating training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  9,  9,  5,  5,  5,  5, 12, 12, 12, 12, 12,  0,  0,  0,  0,\n",
       "         0,  0,  5,  5,  5,  5,  5,  5, 10, 10, 10, 10, 10, 10,  0,  0,\n",
       "         0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  6,  6,  6,  6,  6,  6,\n",
       "         5,  5,  5,  5,  5,  5,  2,  2,  2,  2,  2,  2, 11, 11, 11, 11,\n",
       "        11, 11,  7,  7,  7,  7,  7,  7,  4,  4,  4,  4,  4,  8,  8,  8,\n",
       "         8,  3,  3,  3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Training Process\n",
    "After generating training data, let’s move on to the model. Similar to the majority of neural network models, the steps to train the word2vec model are initializing weights (parameters that we want to train), propagating forward, calculating the cost, propagating backward and updating the weights. The whole process will be repeated for several iterations based on how many epochs we want to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of parameters to be trained\n",
    "There are two layers in the model needed to be initialized and trained, the word embedding layer and the dense layer.\n",
    "\n",
    "The shape of the word embedding will be (vocab_size, emb_size) . Why is that? If we’d like to use a vector with emb_size elements to represent a vocabulary and the total number of vocabularies our corpus is vocab_size, then we can represent all the vocabularies with a vocab_size x emb_size matrix with each row representing a word.\n",
    "\n",
    "The shape of the dense layer will be (vocab_size, emb_size) . How come? The operation that would be performed in this layer is a matrix multiplication. The input of this layer will be (emb_size, # of training instances)and we’d like the output to be (vocab_size, # of training instances)(For each word, we would like to know what the probability that the word appears with the given input word). Note: I do not include biases in the dense layer.\n",
    "\n",
    "The following is the code for initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/5/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are three steps in the forward propagation, obtaining input word’s vector representation from word embedding, passing the vector to the dense layer and then applying softmax function to the output of the dense layer.\n",
    "\n",
    "In some literatures, the input is presented as a one-hot vector (Let’s say an one-hot vector with i-th element being 1). By multiplying the word embedding matrix and the one-hot vector, we can get the vector representing the input word. However, the result of performing matrix multiplication is essentially the same as selecting the ith row of the word embedding matrix. We can save lots of computational time by simply selecting the row associating with the input word.\n",
    "\n",
    "The rest of the process is just a multi-class linear regression model.\n",
    "\n",
    "The following graph could be used to recall the main operation of the dense layer.\n",
    "\n",
    "<img src='asset/5/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we apply softmax function to the output of the dense layer which gives us the probability of each word appearing near the given input word. The following equation could be used to remind what softmax function is.\n",
    "\n",
    "<img src='asset/5/6.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is code for forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of Cost (L)\n",
    "Here, we would use cross entropy to calculate cost:\n",
    "\n",
    "<img src='asset/5/7.png'>\n",
    "\n",
    "The following is code for cost computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass (Back propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='asset/5/8.png'>\n",
    "\n",
    "During the back propagation process, we would like to calculate gradients of the trainable weights with respect to the loss function and update the weight with its associated gradient. Back propagation is the methods used to calculate those gradients. It is nothing fancy but chain rule in Calculus:\n",
    "\n",
    "<img src='asset/5/9.png'>\n",
    "\n",
    "It is the weights in the dense layer and the word embedding layer that we would like to train. Therefore we need to calculate gradients for those weights:\n",
    "\n",
    "<img src='asset/5/10.png'>\n",
    "\n",
    "<img src='asset/5/11.png'>\n",
    "\n",
    "<img src='asset/5/12.png'>\n",
    "\n",
    "The next step is to update the weights with the following formula:\n",
    "\n",
    "<img src='asset/5/13.png'>\n",
    "\n",
    "The following is code for backward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "To train the model, repeat the process of forward propagation, backward propagation and weight updating. During the training, the cost after each epoch should have decreasing trend.\n",
    "\n",
    "The following is the code for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encodeing of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.5520810731635035\n",
      "Cost after epoch 10: 2.551838594047321\n",
      "Cost after epoch 20: 2.5515813624379544\n",
      "Cost after epoch 30: 2.5512920509453636\n",
      "Cost after epoch 40: 2.5509530498815542\n",
      "Cost after epoch 50: 2.550545518749669\n",
      "Cost after epoch 60: 2.5500587266024417\n",
      "Cost after epoch 70: 2.5494658101636207\n",
      "Cost after epoch 80: 2.548741917807595\n",
      "Cost after epoch 90: 2.547860021055674\n",
      "Cost after epoch 100: 2.546789458014371\n",
      "Cost after epoch 110: 2.5455207482443654\n",
      "Cost after epoch 120: 2.5440022558033846\n",
      "Cost after epoch 130: 2.5421893092281724\n",
      "Cost after epoch 140: 2.5400341202618635\n",
      "Cost after epoch 150: 2.537483079084205\n",
      "Cost after epoch 160: 2.534534929224933\n",
      "Cost after epoch 170: 2.5310932354044\n",
      "Cost after epoch 180: 2.527086237174343\n",
      "Cost after epoch 190: 2.5224442130472244\n",
      "Cost after epoch 200: 2.517096087077651\n",
      "Cost after epoch 210: 2.5110898308493455\n",
      "Cost after epoch 220: 2.504291184761226\n",
      "Cost after epoch 230: 2.496641401825204\n",
      "Cost after epoch 240: 2.488111940031918\n",
      "Cost after epoch 250: 2.4787013714244672\n",
      "Cost after epoch 260: 2.468635767478468\n",
      "Cost after epoch 270: 2.457851021147673\n",
      "Cost after epoch 280: 2.446448429134716\n",
      "Cost after epoch 290: 2.4345974293938344\n",
      "Cost after epoch 300: 2.422511578101829\n",
      "Cost after epoch 310: 2.410652243971134\n",
      "Cost after epoch 320: 2.3990700615631084\n",
      "Cost after epoch 330: 2.38796909688871\n",
      "Cost after epoch 340: 2.377537255428377\n",
      "Cost after epoch 350: 2.36789853788292\n",
      "Cost after epoch 360: 2.359249749158586\n",
      "Cost after epoch 370: 2.3513921824537594\n",
      "Cost after epoch 380: 2.3442096129863303\n",
      "Cost after epoch 390: 2.3375570670097336\n",
      "Cost after epoch 400: 2.331263425373489\n",
      "Cost after epoch 410: 2.32526072879919\n",
      "Cost after epoch 420: 2.3192857429148948\n",
      "Cost after epoch 430: 2.313186067288073\n",
      "Cost after epoch 440: 2.3068458202089133\n",
      "Cost after epoch 450: 2.3001761464033126\n",
      "Cost after epoch 460: 2.293246812218541\n",
      "Cost after epoch 470: 2.28592392045111\n",
      "Cost after epoch 480: 2.2781850561363166\n",
      "Cost after epoch 490: 2.270042492103589\n",
      "Cost after epoch 500: 2.2615265338469004\n",
      "Cost after epoch 510: 2.252844270281172\n",
      "Cost after epoch 520: 2.2439174776176105\n",
      "Cost after epoch 530: 2.2347897683642146\n",
      "Cost after epoch 540: 2.2255272325102933\n",
      "Cost after epoch 550: 2.216198597969469\n",
      "Cost after epoch 560: 2.207040837962673\n",
      "Cost after epoch 570: 2.197968552405182\n",
      "Cost after epoch 580: 2.1890236507284717\n",
      "Cost after epoch 590: 2.180262329280059\n",
      "Cost after epoch 600: 2.171735146674047\n",
      "Cost after epoch 610: 2.1636323011139202\n",
      "Cost after epoch 620: 2.1558478956978453\n",
      "Cost after epoch 630: 2.148392308308054\n",
      "Cost after epoch 640: 2.1412853739085196\n",
      "Cost after epoch 650: 2.1345396809945756\n",
      "Cost after epoch 660: 2.1282730001544232\n",
      "Cost after epoch 670: 2.122371606998895\n",
      "Cost after epoch 680: 2.1168170578104193\n",
      "Cost after epoch 690: 2.1116000186931636\n",
      "Cost after epoch 700: 2.1067079788058267\n",
      "Cost after epoch 710: 2.102206304060172\n",
      "Cost after epoch 720: 2.097996586186518\n",
      "Cost after epoch 730: 2.094053144711394\n",
      "Cost after epoch 740: 2.090359543203733\n",
      "Cost after epoch 750: 2.086899493276437\n",
      "Cost after epoch 760: 2.08371399158279\n",
      "Cost after epoch 770: 2.080730025170582\n",
      "Cost after epoch 780: 2.077927385841698\n",
      "Cost after epoch 790: 2.075293450879126\n",
      "Cost after epoch 800: 2.072816490098424\n",
      "Cost after epoch 810: 2.070526636691975\n",
      "Cost after epoch 820: 2.068372560365436\n",
      "Cost after epoch 830: 2.066340820716916\n",
      "Cost after epoch 840: 2.064423538234313\n",
      "Cost after epoch 850: 2.0626135131444703\n",
      "Cost after epoch 860: 2.060934300043606\n",
      "Cost after epoch 870: 2.0593496644557576\n",
      "Cost after epoch 880: 2.057850899434871\n",
      "Cost after epoch 890: 2.056433282432151\n",
      "Cost after epoch 900: 2.0550924948329916\n",
      "Cost after epoch 910: 2.0538469774755117\n",
      "Cost after epoch 920: 2.0526706689496566\n",
      "Cost after epoch 930: 2.051557811021232\n",
      "Cost after epoch 940: 2.0505055323391397\n",
      "Cost after epoch 950: 2.049511197580914\n",
      "Cost after epoch 960: 2.0485889863835696\n",
      "Cost after epoch 970: 2.047719956932021\n",
      "Cost after epoch 980: 2.0469002236075187\n",
      "Cost after epoch 990: 2.0461280043714742\n",
      "Cost after epoch 1000: 2.045401653960943\n",
      "Cost after epoch 1010: 2.0447317101535547\n",
      "Cost after epoch 1020: 2.0441044502951935\n",
      "Cost after epoch 1030: 2.043517198288868\n",
      "Cost after epoch 1040: 2.042968781770364\n",
      "Cost after epoch 1050: 2.042458093814336\n",
      "Cost after epoch 1060: 2.0419924550992237\n",
      "Cost after epoch 1070: 2.04156209102273\n",
      "Cost after epoch 1080: 2.0411650742434606\n",
      "Cost after epoch 1090: 2.040800503328726\n",
      "Cost after epoch 1100: 2.040467492628635\n",
      "Cost after epoch 1110: 2.0401704923024515\n",
      "Cost after epoch 1120: 2.0399027879459593\n",
      "Cost after epoch 1130: 2.0396628963391343\n",
      "Cost after epoch 1140: 2.0394499812392186\n",
      "Cost after epoch 1150: 2.0392631978944\n",
      "Cost after epoch 1160: 2.0391045223012143\n",
      "Cost after epoch 1170: 2.038969666307527\n",
      "Cost after epoch 1180: 2.0388574300079605\n",
      "Cost after epoch 1190: 2.0387669659156065\n",
      "Cost after epoch 1200: 2.0386974146050143\n",
      "Cost after epoch 1210: 2.038648758879495\n",
      "Cost after epoch 1220: 2.038618652793341\n",
      "Cost after epoch 1230: 2.038606118475194\n",
      "Cost after epoch 1240: 2.038610306057633\n",
      "Cost after epoch 1250: 2.038630360738462\n",
      "Cost after epoch 1260: 2.03866479173756\n",
      "Cost after epoch 1270: 2.0387127833221803\n",
      "Cost after epoch 1280: 2.0387735615976847\n",
      "Cost after epoch 1290: 2.0388463144447617\n",
      "Cost after epoch 1300: 2.0389302336606168\n",
      "Cost after epoch 1310: 2.03902285069009\n",
      "Cost after epoch 1320: 2.039124502975263\n",
      "Cost after epoch 1330: 2.039234608537794\n",
      "Cost after epoch 1340: 2.039352430629367\n",
      "Cost after epoch 1350: 2.039477243411122\n",
      "Cost after epoch 1360: 2.039606033087265\n",
      "Cost after epoch 1370: 2.0397399444076387\n",
      "Cost after epoch 1380: 2.039878569187074\n",
      "Cost after epoch 1390: 2.0400212712214456\n",
      "Cost after epoch 1400: 2.040167430137044\n",
      "Cost after epoch 1410: 2.040313842676783\n",
      "Cost after epoch 1420: 2.0404621568570662\n",
      "Cost after epoch 1430: 2.040612116469505\n",
      "Cost after epoch 1440: 2.0407632011677013\n",
      "Cost after epoch 1450: 2.0409149101191937\n",
      "Cost after epoch 1460: 2.041064129334962\n",
      "Cost after epoch 1470: 2.0412127582916257\n",
      "Cost after epoch 1480: 2.0413606721249242\n",
      "Cost after epoch 1490: 2.0415074757758775\n",
      "Cost after epoch 1500: 2.0416527964791986\n",
      "Cost after epoch 1510: 2.0417938128942725\n",
      "Cost after epoch 1520: 2.04193248051526\n",
      "Cost after epoch 1530: 2.0420687876636503\n",
      "Cost after epoch 1540: 2.042202468387776\n",
      "Cost after epoch 1550: 2.042333280412467\n",
      "Cost after epoch 1560: 2.0424588232413092\n",
      "Cost after epoch 1570: 2.04258096714889\n",
      "Cost after epoch 1580: 2.042699795335305\n",
      "Cost after epoch 1590: 2.042815166181205\n",
      "Cost after epoch 1600: 2.0429269608938303\n",
      "Cost after epoch 1610: 2.04303325416963\n",
      "Cost after epoch 1620: 2.043135737931184\n",
      "Cost after epoch 1630: 2.043234568068555\n",
      "Cost after epoch 1640: 2.043329710883002\n",
      "Cost after epoch 1650: 2.043421151963984\n",
      "Cost after epoch 1660: 2.0435074289611412\n",
      "Cost after epoch 1670: 2.0435900027211598\n",
      "Cost after epoch 1680: 2.043669074100438\n",
      "Cost after epoch 1690: 2.043744689342562\n",
      "Cost after epoch 1700: 2.043816908161331\n",
      "Cost after epoch 1710: 2.0438846682635416\n",
      "Cost after epoch 1720: 2.0439491792136475\n",
      "Cost after epoch 1730: 2.0440106556442292\n",
      "Cost after epoch 1740: 2.0440691886111764\n",
      "Cost after epoch 1750: 2.044124875805195\n",
      "Cost after epoch 1760: 2.044176964591929\n",
      "Cost after epoch 1770: 2.044226417865787\n",
      "Cost after epoch 1780: 2.0442734345126525\n",
      "Cost after epoch 1790: 2.0443181155890544\n",
      "Cost after epoch 1800: 2.04436056256981\n",
      "Cost after epoch 1810: 2.0444002396398337\n",
      "Cost after epoch 1820: 2.044437884767335\n",
      "Cost after epoch 1830: 2.044473659024256\n",
      "Cost after epoch 1840: 2.044507646521493\n",
      "Cost after epoch 1850: 2.0445399275446303\n",
      "Cost after epoch 1860: 2.044570108144636\n",
      "Cost after epoch 1870: 2.0445987325171444\n",
      "Cost after epoch 1880: 2.044625913097399\n",
      "Cost after epoch 1890: 2.0446517026834248\n",
      "Cost after epoch 1900: 2.0446761485877687\n",
      "Cost after epoch 1910: 2.044698952497835\n",
      "Cost after epoch 1920: 2.0447204982413356\n",
      "Cost after epoch 1930: 2.044740850433746\n",
      "Cost after epoch 1940: 2.0447600293370307\n",
      "Cost after epoch 1950: 2.0447780504452826\n",
      "Cost after epoch 1960: 2.044794693924352\n",
      "Cost after epoch 1970: 2.044810213383166\n",
      "Cost after epoch 1980: 2.0448246353982453\n",
      "Cost after epoch 1990: 2.0448379565944275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2000: 2.044850171134739\n",
      "Cost after epoch 2010: 2.044861141822219\n",
      "Cost after epoch 2020: 2.044871015934924\n",
      "Cost after epoch 2030: 2.0448797963682614\n",
      "Cost after epoch 2040: 2.0448874701268474\n",
      "Cost after epoch 2050: 2.0448940246118594\n",
      "Cost after epoch 2060: 2.044899418939657\n",
      "Cost after epoch 2070: 2.0449037112194115\n",
      "Cost after epoch 2080: 2.0449068949952602\n",
      "Cost after epoch 2090: 2.044908961526156\n",
      "Cost after epoch 2100: 2.044909904913089\n",
      "Cost after epoch 2110: 2.0449097920979598\n",
      "Cost after epoch 2120: 2.0449086038766198\n",
      "Cost after epoch 2130: 2.0449063351353707\n",
      "Cost after epoch 2140: 2.0449029910412535\n",
      "Cost after epoch 2150: 2.0448985809967586\n",
      "Cost after epoch 2160: 2.044893280859289\n",
      "Cost after epoch 2170: 2.044887003074634\n",
      "Cost after epoch 2180: 2.0448797491338944\n",
      "Cost after epoch 2190: 2.044871541597239\n",
      "Cost after epoch 2200: 2.044862407421022\n",
      "Cost after epoch 2210: 2.0448526189541507\n",
      "Cost after epoch 2220: 2.0448420289088047\n",
      "Cost after epoch 2230: 2.044830645433419\n",
      "Cost after epoch 2240: 2.044818506151091\n",
      "Cost after epoch 2250: 2.0448056522399107\n",
      "Cost after epoch 2260: 2.0447924284661516\n",
      "Cost after epoch 2270: 2.044778637974025\n",
      "Cost after epoch 2280: 2.044764292104238\n",
      "Cost after epoch 2290: 2.044749437372189\n",
      "Cost after epoch 2300: 2.0447341224559468\n",
      "Cost after epoch 2310: 2.044718733845104\n",
      "Cost after epoch 2320: 2.044703037530793\n",
      "Cost after epoch 2330: 2.044687043241035\n",
      "Cost after epoch 2340: 2.044670798854185\n",
      "Cost after epoch 2350: 2.044654352923351\n",
      "Cost after epoch 2360: 2.0446381022400053\n",
      "Cost after epoch 2370: 2.044621789718111\n",
      "Cost after epoch 2380: 2.044605419116514\n",
      "Cost after epoch 2390: 2.044589032839107\n",
      "Cost after epoch 2400: 2.04457267270508\n",
      "Cost after epoch 2410: 2.044556718619781\n",
      "Cost after epoch 2420: 2.044540904660955\n",
      "Cost after epoch 2430: 2.044525225701825\n",
      "Cost after epoch 2440: 2.0445097137206925\n",
      "Cost after epoch 2450: 2.0444943992387272\n",
      "Cost after epoch 2460: 2.044479625491556\n",
      "Cost after epoch 2470: 2.04446513033893\n",
      "Cost after epoch 2480: 2.0444508985272205\n",
      "Cost after epoch 2490: 2.044436948917548\n",
      "Cost after epoch 2500: 2.04442329845392\n",
      "Cost after epoch 2510: 2.0444102426413244\n",
      "Cost after epoch 2520: 2.04439753259217\n",
      "Cost after epoch 2530: 2.0443851431035243\n",
      "Cost after epoch 2540: 2.0443730792994543\n",
      "Cost after epoch 2550: 2.044361344284877\n",
      "Cost after epoch 2560: 2.0443501833333313\n",
      "Cost after epoch 2570: 2.044339366194632\n",
      "Cost after epoch 2580: 2.0443288588464403\n",
      "Cost after epoch 2590: 2.0443186536603637\n",
      "Cost after epoch 2600: 2.0443087411561183\n",
      "Cost after epoch 2610: 2.044299321289978\n",
      "Cost after epoch 2620: 2.0442901834753493\n",
      "Cost after epoch 2630: 2.044281286474588\n",
      "Cost after epoch 2640: 2.044272611955703\n",
      "Cost after epoch 2650: 2.0442641400782815\n",
      "Cost after epoch 2660: 2.0442560361363937\n",
      "Cost after epoch 2670: 2.0442481061776063\n",
      "Cost after epoch 2680: 2.044240303439222\n",
      "Cost after epoch 2690: 2.044232601503312\n",
      "Cost after epoch 2700: 2.0442249728809734\n",
      "Cost after epoch 2710: 2.044217562744718\n",
      "Cost after epoch 2720: 2.044210187400383\n",
      "Cost after epoch 2730: 2.044202796070461\n",
      "Cost after epoch 2740: 2.0441953570377223\n",
      "Cost after epoch 2750: 2.04418783797993\n",
      "Cost after epoch 2760: 2.044180380577287\n",
      "Cost after epoch 2770: 2.0441728026053823\n",
      "Cost after epoch 2780: 2.0441650504509807\n",
      "Cost after epoch 2790: 2.0441570897604997\n",
      "Cost after epoch 2800: 2.044148886021929\n",
      "Cost after epoch 2810: 2.0441405945522364\n",
      "Cost after epoch 2820: 2.0441320243975776\n",
      "Cost after epoch 2830: 2.044123119898046\n",
      "Cost after epoch 2840: 2.044113846409694\n",
      "Cost after epoch 2850: 2.0441041695254167\n",
      "Cost after epoch 2860: 2.0440942745289936\n",
      "Cost after epoch 2870: 2.044083951080138\n",
      "Cost after epoch 2880: 2.0440731418789415\n",
      "Cost after epoch 2890: 2.0440618139108357\n",
      "Cost after epoch 2900: 2.044049934723681\n",
      "Cost after epoch 2910: 2.0440377341219342\n",
      "Cost after epoch 2920: 2.044024972090644\n",
      "Cost after epoch 2930: 2.044011589764978\n",
      "Cost after epoch 2940: 2.04399755721062\n",
      "Cost after epoch 2950: 2.0439828453040634\n",
      "Cost after epoch 2960: 2.0439677404887377\n",
      "Cost after epoch 2970: 2.043951963813566\n",
      "Cost after epoch 2980: 2.043935454667116\n",
      "Cost after epoch 2990: 2.0439181871844285\n",
      "Cost after epoch 3000: 2.043900136483191\n",
      "Cost after epoch 3010: 2.043881654958719\n",
      "Cost after epoch 3020: 2.043862416963289\n",
      "Cost after epoch 3030: 2.043842359784472\n",
      "Cost after epoch 3040: 2.043821462195394\n",
      "Cost after epoch 3050: 2.0437997040534563\n",
      "Cost after epoch 3060: 2.0437775100899565\n",
      "Cost after epoch 3070: 2.043754501996053\n",
      "Cost after epoch 3080: 2.0437306145135437\n",
      "Cost after epoch 3090: 2.043705831264162\n",
      "Cost after epoch 3100: 2.043680136995196\n",
      "Cost after epoch 3110: 2.04365403230158\n",
      "Cost after epoch 3120: 2.0436270821792095\n",
      "Cost after epoch 3130: 2.043599218362044\n",
      "Cost after epoch 3140: 2.0435704292445975\n",
      "Cost after epoch 3150: 2.0435407043393727\n",
      "Cost after epoch 3160: 2.043510620992825\n",
      "Cost after epoch 3170: 2.043479685401076\n",
      "Cost after epoch 3180: 2.0434478258703437\n",
      "Cost after epoch 3190: 2.043415035278647\n",
      "Cost after epoch 3200: 2.0433813075773433\n",
      "Cost after epoch 3210: 2.0433472954251157\n",
      "Cost after epoch 3220: 2.0433124459373873\n",
      "Cost after epoch 3230: 2.0432766836586618\n",
      "Cost after epoch 3240: 2.0432400055156084\n",
      "Cost after epoch 3250: 2.0432024094374968\n",
      "Cost after epoch 3260: 2.0431646199953595\n",
      "Cost after epoch 3270: 2.0431260265847375\n",
      "Cost after epoch 3280: 2.0430865497685033\n",
      "Cost after epoch 3290: 2.0430461900005725\n",
      "Cost after epoch 3300: 2.043004948650288\n",
      "Cost after epoch 3310: 2.042963617226905\n",
      "Cost after epoch 3320: 2.0429215301784454\n",
      "Cost after epoch 3330: 2.0428786039941444\n",
      "Cost after epoch 3340: 2.0428348420955964\n",
      "Cost after epoch 3350: 2.0427902487241654\n",
      "Cost after epoch 3360: 2.0427456761590053\n",
      "Cost after epoch 3370: 2.042700407829633\n",
      "Cost after epoch 3380: 2.0426543561894617\n",
      "Cost after epoch 3390: 2.042607527067378\n",
      "Cost after epoch 3400: 2.042559927014124\n",
      "Cost after epoch 3410: 2.0425124620715263\n",
      "Cost after epoch 3420: 2.042464369475935\n",
      "Cost after epoch 3430: 2.0424155578052208\n",
      "Cost after epoch 3440: 2.042366034761367\n",
      "Cost after epoch 3450: 2.0423158086726407\n",
      "Cost after epoch 3460: 2.0422658318162092\n",
      "Cost after epoch 3470: 2.042215300705788\n",
      "Cost after epoch 3480: 2.0421641203125485\n",
      "Cost after epoch 3490: 2.042112299721548\n",
      "Cost after epoch 3500: 2.0420598485540653\n",
      "Cost after epoch 3510: 2.0420077575511395\n",
      "Cost after epoch 3520: 2.041955188387947\n",
      "Cost after epoch 3530: 2.041902042788451\n",
      "Cost after epoch 3540: 2.0418483307854642\n",
      "Cost after epoch 3550: 2.0417940628654443\n",
      "Cost after epoch 3560: 2.041740260448709\n",
      "Cost after epoch 3570: 2.041686056463934\n",
      "Cost after epoch 3580: 2.0416313498181764\n",
      "Cost after epoch 3590: 2.0415761511161135\n",
      "Cost after epoch 3600: 2.0415204713420363\n",
      "Cost after epoch 3610: 2.0414653549847057\n",
      "Cost after epoch 3620: 2.0414099123500558\n",
      "Cost after epoch 3630: 2.041354040009295\n",
      "Cost after epoch 3640: 2.0412977488229256\n",
      "Cost after epoch 3650: 2.041241049965934\n",
      "Cost after epoch 3660: 2.0411850037041237\n",
      "Cost after epoch 3670: 2.041128703730274\n",
      "Cost after epoch 3680: 2.041072044792481\n",
      "Cost after epoch 3690: 2.0410150377483323\n",
      "Cost after epoch 3700: 2.0409576937135196\n",
      "Cost after epoch 3710: 2.0409010818674544\n",
      "Cost after epoch 3720: 2.0408442850687654\n",
      "Cost after epoch 3730: 2.0407871967695996\n",
      "Cost after epoch 3740: 2.040729827618712\n",
      "Cost after epoch 3750: 2.040672188474861\n",
      "Cost after epoch 3760: 2.0406153510716507\n",
      "Cost after epoch 3770: 2.040558392893024\n",
      "Cost after epoch 3780: 2.040501206621799\n",
      "Cost after epoch 3790: 2.0404438025392397\n",
      "Cost after epoch 3800: 2.040386191096125\n",
      "Cost after epoch 3810: 2.0403294407733146\n",
      "Cost after epoch 3820: 2.0402726287556647\n",
      "Cost after epoch 3830: 2.0402156474702537\n",
      "Cost after epoch 3840: 2.040158506713452\n",
      "Cost after epoch 3850: 2.040101216417465\n",
      "Cost after epoch 3860: 2.0400448365814925\n",
      "Cost after epoch 3870: 2.039988448740193\n",
      "Cost after epoch 3880: 2.039931945555489\n",
      "Cost after epoch 3890: 2.0398753362566553\n",
      "Cost after epoch 3900: 2.039818630181115\n",
      "Cost after epoch 3910: 2.039762874202484\n",
      "Cost after epoch 3920: 2.0397071584027957\n",
      "Cost after epoch 3930: 2.0396513761387807\n",
      "Cost after epoch 3940: 2.039595536019829\n",
      "Cost after epoch 3950: 2.0395396467409963\n",
      "Cost after epoch 3960: 2.039484737983755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 3970: 2.0394299121142083\n",
      "Cost after epoch 3980: 2.03937506360777\n",
      "Cost after epoch 3990: 2.0393202004252533\n",
      "Cost after epoch 4000: 2.0392653305950725\n",
      "Cost after epoch 4010: 2.0392114630985865\n",
      "Cost after epoch 4020: 2.0391577158643353\n",
      "Cost after epoch 4030: 2.0391039848706143\n",
      "Cost after epoch 4040: 2.0390502774202695\n",
      "Cost after epoch 4050: 2.0389966008694063\n",
      "Cost after epoch 4060: 2.038943940522893\n",
      "Cost after epoch 4070: 2.0388914327037266\n",
      "Cost after epoch 4080: 2.038838975235649\n",
      "Cost after epoch 4090: 2.0387865747692433\n",
      "Cost after epoch 4100: 2.0387342379971103\n",
      "Cost after epoch 4110: 2.0386829240701445\n",
      "Cost after epoch 4120: 2.038631790110593\n",
      "Cost after epoch 4130: 2.0385807360879142\n",
      "Cost after epoch 4140: 2.0385297680176206\n",
      "Cost after epoch 4150: 2.0384788919485284\n",
      "Cost after epoch 4160: 2.038429038862641\n",
      "Cost after epoch 4170: 2.0383793886832238\n",
      "Cost after epoch 4180: 2.0383298437838535\n",
      "Cost after epoch 4190: 2.038280409570583\n",
      "Cost after epoch 4200: 2.0382310914761073\n",
      "Cost after epoch 4210: 2.038182790722857\n",
      "Cost after epoch 4220: 2.038134711661486\n",
      "Cost after epoch 4230: 2.0380867592874887\n",
      "Cost after epoch 4240: 2.0380389384289375\n",
      "Cost after epoch 4250: 2.037991253935532\n",
      "Cost after epoch 4260: 2.0379445760647172\n",
      "Cost after epoch 4270: 2.037898134875166\n",
      "Cost after epoch 4280: 2.0378518381633643\n",
      "Cost after epoch 4290: 2.03780569021468\n",
      "Cost after epoch 4300: 2.037759695332386\n",
      "Cost after epoch 4310: 2.037714691950187\n",
      "Cost after epoch 4320: 2.037669936800244\n",
      "Cost after epoch 4330: 2.0376253406227125\n",
      "Cost after epoch 4340: 2.0375809071976567\n",
      "Cost after epoch 4350: 2.0375366403203197\n",
      "Cost after epoch 4360: 2.037493346051107\n",
      "Cost after epoch 4370: 2.037450308476579\n",
      "Cost after epoch 4380: 2.0374074413894743\n",
      "Cost after epoch 4390: 2.0373647481027612\n",
      "Cost after epoch 4400: 2.037322231942622\n",
      "Cost after epoch 4410: 2.0372806663200285\n",
      "Cost after epoch 4420: 2.0372393631022563\n",
      "Cost after epoch 4430: 2.0371982392125783\n",
      "Cost after epoch 4440: 2.0371572975348733\n",
      "Cost after epoch 4450: 2.037116540964838\n",
      "Cost after epoch 4460: 2.0370767102276117\n",
      "Cost after epoch 4470: 2.0370371451717237\n",
      "Cost after epoch 4480: 2.036997765900851\n",
      "Cost after epoch 4490: 2.036958574906834\n",
      "Cost after epoch 4500: 2.036919574692354\n",
      "Cost after epoch 4510: 2.0368814734686773\n",
      "Cost after epoch 4520: 2.0368436390676767\n",
      "Cost after epoch 4530: 2.036805994798087\n",
      "Cost after epoch 4540: 2.0367685427952726\n",
      "Cost after epoch 4550: 2.036731285204754\n",
      "Cost after epoch 4560: 2.0366948980753183\n",
      "Cost after epoch 4570: 2.0366587770514117\n",
      "Cost after epoch 4580: 2.036622848649072\n",
      "Cost after epoch 4590: 2.036587114680892\n",
      "Cost after epoch 4600: 2.0365515769691456\n",
      "Cost after epoch 4610: 2.0365168799045033\n",
      "Cost after epoch 4620: 2.0364824466240803\n",
      "Cost after epoch 4630: 2.0364482068337546\n",
      "Cost after epoch 4640: 2.036414162054975\n",
      "Cost after epoch 4650: 2.0363803138185266\n",
      "Cost after epoch 4660: 2.0363472754902947\n",
      "Cost after epoch 4670: 2.0363144972528264\n",
      "Cost after epoch 4680: 2.0362819119672184\n",
      "Cost after epoch 4690: 2.03624952089321\n",
      "Cost after epoch 4700: 2.036217325299608\n",
      "Cost after epoch 4710: 2.0361859082681653\n",
      "Cost after epoch 4720: 2.0361547464721585\n",
      "Cost after epoch 4730: 2.036123775878516\n",
      "Cost after epoch 4740: 2.0360929975124744\n",
      "Cost after epoch 4750: 2.036062412408121\n",
      "Cost after epoch 4760: 2.0360325741916405\n",
      "Cost after epoch 4770: 2.0360029853828623\n",
      "Cost after epoch 4780: 2.0359735849926084\n",
      "Cost after epoch 4790: 2.0359443738366316\n",
      "Cost after epoch 4800: 2.035915352739338\n",
      "Cost after epoch 4810: 2.035887046770534\n",
      "Cost after epoch 4820: 2.0358589835791325\n",
      "Cost after epoch 4830: 2.0358311051473907\n",
      "Cost after epoch 4840: 2.035803412104482\n",
      "Cost after epoch 4850: 2.0357759050880446\n",
      "Cost after epoch 4860: 2.0357490815659762\n",
      "Cost after epoch 4870: 2.035722493539984\n",
      "Cost after epoch 4880: 2.0356960858826314\n",
      "Cost after epoch 4890: 2.035669859057416\n",
      "Cost after epoch 4900: 2.0356438135361077\n",
      "Cost after epoch 4910: 2.035618420180894\n",
      "Cost after epoch 4920: 2.0355932545240463\n",
      "Cost after epoch 4930: 2.035568264240312\n",
      "Cost after epoch 4940: 2.0355434496465383\n",
      "Cost after epoch 4950: 2.0355188110676465\n",
      "Cost after epoch 4960: 2.0354947937861314\n",
      "Cost after epoch 4970: 2.035470996008057\n",
      "Cost after epoch 4980: 2.0354473681167393\n",
      "Cost after epoch 4990: 2.0354239102996585\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After train the model with data generated from the example sentence above with a window size of 3 for 5000 epochs (with a simple learning rate decay), we can see the model can output most neighboring words given each word as an input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', \"loser's\"]\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'market']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "beating's neighbor words: ['market', 'stock', 'investing', 'costs']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization (Space)\n",
    "When I tried to train the model above with a larger dataset, I found the memory consumption kept increasing during the training process and the python kernel finally shut down. Later on, I figured out the issue had to do with the way I fed the labels Y into the model.\n",
    "\n",
    "In the original code, each label is a one hot vector which used a bunch of zeros and a single one to represent the labeled output word. When the vocabulary size grows bigger, we waste so much memory to the zeros that do not provide us useful information.\n",
    "\n",
    "The memory consumption problem goes away after I start to feed the label with its associated word ind only. We have decreased the space from O(vocabulary size * m) to O(m).\n",
    "\n",
    "The following is my code implementation (There are only 2 places needed to be changed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.log(softmax_out[Y.flatten(), np.arange(Y.shape[1])] + 0.001))\n",
    "    return cost\n",
    "  \n",
    "\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    softmax_out[Y.flatten(), np.arange(m)] -= 1.0\n",
    "    dL_dZ = softmax_out\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
