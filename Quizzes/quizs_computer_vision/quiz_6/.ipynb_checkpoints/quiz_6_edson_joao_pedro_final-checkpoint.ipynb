{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvyMZaXxqKNF"
   },
   "source": [
    "# Quiz 6 - Object Recognition: BoF vs ConvNets\n",
    "#### Edson Roteia Araujo Junior e Jo√£o Pedro Moreira Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmdijAdVqKNH"
   },
   "source": [
    "### Instructions\n",
    "The goal of this quiz is to implement two object recognition approaches:\n",
    "\n",
    "1.     A classifier based on bag of features:\n",
    "\n",
    "    - Use SVM or Random Forest as classifiers.\n",
    "\n",
    "    - Try different sizes for the dictionary.\n",
    "\n",
    "2.     A classifier using ConvNet implemented in Keras:\n",
    " \n",
    "     - Use an architecture inspired in the LeNet5\n",
    "\n",
    "Your code must be implemented on a notebook python and you must use the CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html) for training and testing.\n",
    "\n",
    "The notebook must present a confusion matrix and the average accuracy for each approach. You also have to report both training and test accuracies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0UCLTOyqKNI"
   },
   "source": [
    "#### In order to enable the SIFT module of the OpenCV library.<span style=\"color:red\"> We install the opencv-contrib-python at version 3.4.2.16.</span> You should install it to make this notebook work. You can install it through any python package manager, since the module is in a pypi repository. More informations [here](https://pypi.org/project/opencv-contrib-python/3.4.2.16/) ####\n",
    "\n",
    "#### Also you may need to install the <span style=\"color:red\">scikit-image</span> module. Again it can be installed by your python package manager ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python==3.4.2.16 in /anaconda3/envs/cv/lib/python3.7/site-packages (3.4.2.16)\r\n",
      "Requirement already satisfied: numpy>=1.14.5 in /anaconda3/envs/cv/lib/python3.7/site-packages (from opencv-contrib-python==3.4.2.16) (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python==3.4.2.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "pWUCuJwoqKNL",
    "outputId": "e554a729-b346-4cbb-d2ab-e29905bd0f36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from skimage import img_as_ubyte\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "pWUCuJwoqKNL",
    "outputId": "e554a729-b346-4cbb-d2ab-e29905bd0f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 50000\n",
      "Test Data Size: 10000\n",
      "Number of Classes: 10\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train_not_categorical = y_train.copy()\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test_not_categorical = y_test.copy()\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "print(\"Train Data Size: %d\" % len(X_train))\n",
    "print(\"Test Data Size: %d\" % len(X_test))\n",
    "print(\"Number of Classes: %d\" % num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = cv2.xfeatures2d.SIFT_create()\n",
    "extract = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANN Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = {}  # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "bow = cv2.BOWKMeansTrainer(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding descriptors to BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in X_train:\n",
    "    img = img_as_ubyte(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    kp, des_img = extract.detectAndCompute(gray, None)\n",
    "    if des_img is not None:\n",
    "        bow.add(des_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = bow.cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowDiction = cv2.BOWImgDescriptorExtractor(extract, flann)\n",
    "bowDiction.setVocabulary(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting bags from training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bags = []\n",
    "for img in X_train:\n",
    "    img = img_as_ubyte(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    bow_value = np.ravel(bowDiction.compute(gray, detect.detect(img)))\n",
    "    if len(bow_value) < 10:\n",
    "        bow_value = np.zeros(10)\n",
    "    train_bags.append(bow_value)\n",
    "train_bags = np.asarray(train_bags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting bags from test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bags = []\n",
    "for img in X_test:\n",
    "    img = img_as_ubyte(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    bow_value = np.ravel(bowDiction.compute(gray, detect.detect(img)))\n",
    "    if len(bow_value) < 10:\n",
    "        bow_value = np.zeros(10)\n",
    "    test_bags.append(bow_value)\n",
    "test_bags = np.asarray(test_bags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_RF = {\"n_estimators\": 50, \"max_depth\": 15, \"min_samples_leaf\": 0.2, }\n",
    "rfc = RandomForestClassifier()\n",
    "# tune the hyperparameters via a randomized search\n",
    "grid = RandomizedSearchCV(rfc, params_RF)\n",
    "grid.fit(train_bags, y_train)\n",
    "acc = grid.score(test_bags, y_test)\n",
    "print(\"[INFO] grid search accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_SVM = {\"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \"decision_function_shape\": [\"ovo\", \"ovr\", None]}\n",
    "clf = svm.SVC()\n",
    "# tune the hyperparameters via a randomized search\n",
    "grid = RandomizedSearchCV(clf, params_SVM)\n",
    "start = time.time()\n",
    "grid.fit(train_bags, y_train_not_categorical)\n",
    "#print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "acc = grid.score(test_bags, y_test_not_categorical)\n",
    "print(\"[INFO] grid search accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrg1IalsqKNQ"
   },
   "outputs": [],
   "source": [
    "def featureExtraction(sift,image):\n",
    "    image = img_as_ubyte(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "I-1lK7BhqKNZ",
    "outputId": "d5094832-d66b-4bbb-c847-8999b3e5f85c"
   },
   "outputs": [],
   "source": [
    "train_descriptors_list = []\n",
    "for image in X_train:\n",
    "    keypoints, descriptors = featureExtraction(sift,image)\n",
    "    train_descriptors_list.append(descriptors)\n",
    "    \n",
    "print(\"Number of training images with descriptors: %d\" % sum(x is not None for x in train_descriptors_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descriptors_list = []\n",
    "for image in X_test:\n",
    "    keypoints, descriptors = featureExtraction(sift,image)\n",
    "    test_descriptors_list.append(descriptors)\n",
    "    \n",
    "print(\"Number of test images with descriptors: %d\" % sum(x is not None for x in test_descriptors_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-aW0RJG1w4Kh"
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "mjCZASKbqKNf",
    "outputId": "09f437aa-4069-4246-9efd-5659f4a1d1f4"
   },
   "outputs": [],
   "source": [
    "def build_histogram(descriptor_list, cluster_alg):\n",
    "    histogram = np.zeros(len(cluster_alg.cluster_centers_))\n",
    "    cluster_result =  cluster_alg.predict(descriptor_list)\n",
    "    for i in cluster_result:\n",
    "        histogram[i] += 1.0\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "mjCZASKbqKNf",
    "outputId": "09f437aa-4069-4246-9efd-5659f4a1d1f4"
   },
   "outputs": [],
   "source": [
    "flatten_descriptors_list = []\n",
    "for sublist in train_descriptors_list:\n",
    "    if not sublist is None:  \n",
    "      for item in sublist:\n",
    "          flatten_descriptors_list.append(item)\n",
    "\n",
    "print(\"Number of descriptors in total: %d\" % len(flatten_descriptors_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "mjCZASKbqKNf",
    "outputId": "09f437aa-4069-4246-9efd-5659f4a1d1f4"
   },
   "outputs": [],
   "source": [
    "def get_bovws(k):\n",
    "  kmeans = MiniBatchKMeans(n_clusters = k, verbose = False, max_iter = 5000, batch_size = 500)\n",
    "  kmeans.fit(flatten_descriptors_list)\n",
    "  \n",
    "  train_image_bags = []\n",
    "  for image_descriptors in train_descriptors_list:\n",
    "    if(image_descriptors is not None):\n",
    "      histogram = build_histogram(image_descriptors, kmeans)\n",
    "      train_image_bags.append(histogram)\n",
    "    else:\n",
    "      train_image_bags.append(np.zeros((k,)))\n",
    "  \n",
    "  test_image_bags = []\n",
    "  for image_descriptors in test_descriptors_list:\n",
    "    if(image_descriptors is not None):\n",
    "      histogram = build_histogram(image_descriptors, kmeans)\n",
    "      test_image_bags.append(histogram)\n",
    "    else:\n",
    "      test_image_bags.append(np.zeros((k,)))\n",
    "  \n",
    "  return train_image_bags, test_image_bags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "mjCZASKbqKNf",
    "outputId": "09f437aa-4069-4246-9efd-5659f4a1d1f4"
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "train_image_bags, test_image_bags = get_bovws(k)\n",
    "print(train_image_bags[0:5])\n",
    "print(test_image_bags[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv2.BOWKMeansTrainer(100)\n",
    "for desc in flatten_descriptors_list:\n",
    "    bow.add(desc)\n",
    "print 'Added all to BoW'\n",
    "dictionary = bow.cluster()\n",
    "print 'Created clusters'\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = {}  # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "bowDiction = cv2.BOWImgDescriptorExtractor(cv2.xfeatures2d.SIFT_create(), flann)\n",
    "bowDiction.setVocabulary(dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "mjCZASKbqKNf",
    "outputId": "09f437aa-4069-4246-9efd-5659f4a1d1f4"
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcAvNnkT3b7c"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(train_image_bags, y_train)\n",
    "#acc = rfc.score(test_image_bags, y_test)\n",
    "acc = rfc.score(train_image_bags, y_train)\n",
    "print(\"Random Forest Classifier accuracy: {:.2f}%\".format(acc * 100))\n",
    "predictions = rfc.predict(test_image_bags)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC()\n",
    "# clf.fit(image_bags, y_train_not_categorical)\n",
    "# acc = clf.score(image_bags, y_train_not_categorical)\n",
    "# print(\"[INFO] SVM search accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXcaVhXiqKNy"
   },
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qb3C0mMKqKN4"
   },
   "outputs": [],
   "source": [
    "# Plot ad hoc CIFAR10 instances\n",
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "# from scipy.misc import toimage -> DEPRECATED\n",
    "from keras.layers import *\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTw5M7dhqKO9"
   },
   "outputs": [],
   "source": [
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(Image.fromarray(X_train[i]))\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "910LhcIBqKPQ"
   },
   "source": [
    "#### Simple Covolutional Neural Network for CIFAR-10 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-YCz6ymqKPo"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-a0M7nWnqKQR"
   },
   "source": [
    "#### Larger Covolutional Neural Network for CIFAR-10 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0dvoshIqKQS"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3E2Qr2kqKQf"
   },
   "source": [
    "#### Architecture Inspired in the LeNet5 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odcKJDqpqKQg"
   },
   "source": [
    "![lenet5](imgs/lenet5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNfA6DeHqKQi"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(3,32,32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "model.add(Dense(units=84, activation='relu'))\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQxSeMBaqKQn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "quiz_6_edson_joao-pedro.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
