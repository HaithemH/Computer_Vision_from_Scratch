{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../libs')\n",
    "\n",
    "from stnet import STNet\n",
    "from utils import get_images, save_images, list_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stylize(contents_path, styles_path, output_dir, encoder_path, model_path, \n",
    "#             resize_height=None, resize_width=None, suffix=None):\n",
    "\n",
    "#     if isinstance(contents_path, str):\n",
    "#         contents_path = [contents_path]\n",
    "#     if isinstance(styles_path, str):\n",
    "#         styles_path = [styles_path]\n",
    "\n",
    "#     with tf.Graph().as_default(), tf.Session() as sess:\n",
    "#         # build the dataflow graph\n",
    "#         content = tf.placeholder(\n",
    "#             tf.float32, shape=(1, None, None, 3), name='content')\n",
    "#         style   = tf.placeholder(\n",
    "#             tf.float32, shape=(1, None, None, 3), name='style')\n",
    "\n",
    "#         stn = STNet(encoder_path)\n",
    "\n",
    "#         output_image = stn.transform(content, style)\n",
    "\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#         # restore the trained model and run the style transferring\n",
    "#         saver = tf.train.Saver()\n",
    "#         saver.restore(sess, model_path)\n",
    "\n",
    "#         outputs = []\n",
    "#         for content_path in contents_path:\n",
    "\n",
    "#             content_img = get_images(content_path, \n",
    "#                 height=resize_height, width=resize_width)\n",
    "\n",
    "#             for style_path in styles_path:\n",
    "\n",
    "#                 style_img   = get_images(style_path)\n",
    "\n",
    "#                 result = sess.run(output_image, \n",
    "#                     feed_dict={content: content_img, style: style_img})\n",
    "\n",
    "#                 outputs.append(result[0])\n",
    "\n",
    "#     save_images(outputs, contents_path, styles_path, output_dir, suffix=suffix)\n",
    "\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_WEIGHTS = [3.0]\n",
    "CONTENT_WEIGHTS = [1.0]\n",
    "MODEL_SAVE_PATHS = ['../../models/style_weight_2e0.ckpt']\n",
    "\n",
    "# for inferring (stylize)\n",
    "INFERRING_CONTENT_DIR = '../../_inference/content'\n",
    "INFERRING_STYLE_DIR = '../../_inference/style'\n",
    "OUTPUTS_DIR = '../../_inference/output'\n",
    "\n",
    "ENCODER_WEIGHTS_PATH = '../../vgg19_normalised.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_imgs_path = list_images(INFERRING_CONTENT_DIR)\n",
    "style_imgs_path   = list_images(INFERRING_STYLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_path = content_imgs_path\n",
    "styles_path = style_imgs_path\n",
    "output_dir = OUTPUTS_DIR\n",
    "encoder_path = ENCODER_WEIGHTS_PATH\n",
    "model_path = model_save_path\n",
    "suffix = '-' + str(style_weight) + '-' + str(content_weight)\n",
    "resize_height=None\n",
    "resize_width=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_IMAGE_SHAPE = (256, 256, 3)\n",
    "BATCH_SIZE = 8\n",
    "HEIGHT, WIDTH, CHANNELS = TRAINING_IMAGE_SHAPE\n",
    "INPUT_SHAPE = (1, HEIGHT, WIDTH, CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 256, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Begin to stylize images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 12:22:51.935676  5600 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "In[0] and In[1] must have compatible batch dimensions: [1,75,75,64] vs. [1,167,132,64]\n\t [[node attention/MatMul (defined at ../libs\\sanet.py:52) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node attention/MatMul:\n attention/conv2d/BiasAdd (defined at C:\\Users\\USER\\AppData\\Local\\Temp\\tmp23i7stdm.py:56)\n\nOriginal stack trace for 'attention/MatMul':\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-5c3b2b1658c0>\", line 19, in <module>\n    output_image = stn.transform(content, style)\n  File \"../libs\\stnet.py\", line 37, in transform\n    Fcsc_m = self.SAModule.map(enc_c_layers['relu4_1'], enc_c_layers['relu5_1'], enc_s_layers['relu4_1'], enc_s_layers['relu5_1'])\n  File \"../libs\\samod.py\", line 24, in map\n    Fcsc_4 = self.SANet1.map(content_4_1, style_4_1)\n  File \"../libs\\sanet.py\", line 52, in map\n    s = tf.matmul(g, f, transpose_b=True)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2609, in matmul\n    return batch_mat_mul_fn(a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1805, in batch_mat_mul_v2\n    \"BatchMatMulV2\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: In[0] and In[1] must have compatible batch dimensions: [1,75,75,64] vs. [1,167,132,64]\n\t [[{{node attention/MatMul}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5c3b2b1658c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 result = sess.run(output_image, \n\u001b[1;32m---> 38\u001b[1;33m                     feed_dict={content: content_img, style: style_img})\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: In[0] and In[1] must have compatible batch dimensions: [1,75,75,64] vs. [1,167,132,64]\n\t [[node attention/MatMul (defined at ../libs\\sanet.py:52) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node attention/MatMul:\n attention/conv2d/BiasAdd (defined at C:\\Users\\USER\\AppData\\Local\\Temp\\tmp23i7stdm.py:56)\n\nOriginal stack trace for 'attention/MatMul':\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-5c3b2b1658c0>\", line 19, in <module>\n    output_image = stn.transform(content, style)\n  File \"../libs\\stnet.py\", line 37, in transform\n    Fcsc_m = self.SAModule.map(enc_c_layers['relu4_1'], enc_c_layers['relu5_1'], enc_s_layers['relu4_1'], enc_s_layers['relu5_1'])\n  File \"../libs\\samod.py\", line 24, in map\n    Fcsc_4 = self.SANet1.map(content_4_1, style_4_1)\n  File \"../libs\\sanet.py\", line 52, in map\n    s = tf.matmul(g, f, transpose_b=True)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2609, in matmul\n    return batch_mat_mul_fn(a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1805, in batch_mat_mul_v2\n    \"BatchMatMulV2\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "for style_weight, content_weight, model_save_path in zip(STYLE_WEIGHTS, CONTENT_WEIGHTS, MODEL_SAVE_PATHS):\n",
    "    print('\\n>>> Begin to stylize images')\n",
    "    \n",
    "    if isinstance(contents_path, str):\n",
    "        contents_path = [contents_path]\n",
    "    if isinstance(styles_path, str):\n",
    "        styles_path = [styles_path]\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        # build the dataflow graph\n",
    "        content = tf.placeholder(tf.float32, shape=(1, None, None, 3), name='content')\n",
    "        style   = tf.placeholder(tf.float32, shape=(1, None, None, 3), name='style')\n",
    "        \n",
    "#         content = tf.placeholder(tf.float32, shape=INPUT_SHAPE, name='content')\n",
    "#         style   = tf.placeholder(tf.float32, shape=INPUT_SHAPE, name='style')\n",
    "\n",
    "        stn = STNet(encoder_path)\n",
    "\n",
    "        output_image = stn.transform(content, style)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # restore the trained model and run the style transferring\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        outputs = []\n",
    "        for content_path in contents_path:\n",
    "\n",
    "            content_img = get_images(content_path, \n",
    "                height=resize_height, width=resize_width)\n",
    "\n",
    "            for style_path in styles_path:\n",
    "\n",
    "                style_img   = get_images(style_path)\n",
    "\n",
    "                result = sess.run(output_image, \n",
    "                    feed_dict={content: content_img, style: style_img})\n",
    "\n",
    "                outputs.append(result[0])\n",
    "\n",
    "    save_images(outputs, contents_path, styles_path, output_dir, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dir = '../../_input/content/brad_pitt.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic options\n",
    "parser.add_argument('--content', type=str, default = 'input/chicago.jpg',\n",
    "                    help='File path to the content image')\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from os.path import basename\n",
    "from os.path import splitext\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tf = test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content_tf(Image.open(content_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1331, 1054])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "content = content.to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def mean_variance_norm(feat):\n",
    "    size = feat.size()\n",
    "    mean, std = calc_mean_std(feat)\n",
    "    normalized_feat = (feat - mean.expand(size)) / std.expand(size)\n",
    "    return normalized_feat\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 3, (3, 3)),\n",
    ")\n",
    "\n",
    "vgg = nn.Sequential(\n",
    "    nn.Conv2d(3, 3, (1, 1)),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(3, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU()  # relu5-4\n",
    ")\n",
    "\n",
    "class SANet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes):\n",
    "        super(SANet, self).__init__()\n",
    "        self.f = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
    "        self.g = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
    "        self.h = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
    "        self.sm = nn.Softmax(dim = -1)\n",
    "        self.out_conv = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
    "        \n",
    "    def forward(self, content, style):\n",
    "        F = self.f(mean_variance_norm(content))\n",
    "        G = self.g(mean_variance_norm(style))\n",
    "        H = self.h(style)\n",
    "\n",
    "        b, c, h, w = F.size()\n",
    "        F = F.view(b, -1, w * h).permute(0, 2, 1)\n",
    "        \n",
    "        b, c, h, w = G.size()\n",
    "        G = G.view(b, -1, w * h)\n",
    "        \n",
    "        S = torch.bmm(F, G)\n",
    "        S = self.sm(S)\n",
    "        \n",
    "        b, c, h, w = H.size()\n",
    "        H = H.view(b, -1, w * h)\n",
    "        \n",
    "        O = torch.bmm(H, S.permute(0, 2, 1))\n",
    "        b, c, h, w = content.size()\n",
    "        \n",
    "        O = O.view(b, c, h, w)\n",
    "        O = self.out_conv(O)\n",
    "        O += content\n",
    "        return O\n",
    "\n",
    "class Transform(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(Transform, self).__init__()\n",
    "        self.sanet4_1 = SANet(in_planes = in_planes)\n",
    "        self.sanet5_1 = SANet(in_planes = in_planes)\n",
    "        self.upsample5_1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.merge_conv_pad = nn.ReflectionPad2d((1, 1, 1, 1))\n",
    "        self.merge_conv = nn.Conv2d(in_planes, in_planes, (3, 3))\n",
    "    def forward(self, content4_1, style4_1, content5_1, style5_1):\n",
    "        return self.merge_conv(self.merge_conv_pad(self.sanet4_1(content4_1, style4_1) + self.upsample5_1(self.sanet5_1(content5_1, style5_1))))\n",
    "\n",
    "def test_transform():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Basic options\n",
    "parser.add_argument('--content', type=str, default = 'input/chicago.jpg',\n",
    "                    help='File path to the content image')\n",
    "parser.add_argument('--style', type=str, default = 'style/style11.jpg',\n",
    "                    help='File path to the style image, or multiple style \\\n",
    "                    images separated by commas if you want to do style \\\n",
    "                    interpolation or spatial control')\n",
    "parser.add_argument('--steps', type=str, default = 1)\n",
    "parser.add_argument('--vgg', type=str, default = 'vgg_normalised.pth')\n",
    "parser.add_argument('--decoder', type=str, default = 'decoder_iter_500000.pth')\n",
    "parser.add_argument('--transform', type=str, default = 'transformer_iter_500000.pth')\n",
    "\n",
    "# Additional options\n",
    "parser.add_argument('--save_ext', default = '.jpg',\n",
    "                    help='The extension name of the output image')\n",
    "parser.add_argument('--output', type=str, default = 'output',\n",
    "                    help='Directory to save the output image(s)')\n",
    "\n",
    "# Advanced options\n",
    "\n",
    "args = parser.parse_args('')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "\n",
    "decoder = decoder\n",
    "transform = Transform(in_planes = 512)\n",
    "vgg = vgg\n",
    "\n",
    "decoder.eval()\n",
    "transform.eval()\n",
    "vgg.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(args.decoder))\n",
    "transform.load_state_dict(torch.load(args.transform))\n",
    "vgg.load_state_dict(torch.load(args.vgg))\n",
    "\n",
    "norm = nn.Sequential(*list(vgg.children())[:1])\n",
    "enc_1 = nn.Sequential(*list(vgg.children())[:4])  # input -> relu1_1\n",
    "enc_2 = nn.Sequential(*list(vgg.children())[4:11])  # relu1_1 -> relu2_1\n",
    "enc_3 = nn.Sequential(*list(vgg.children())[11:18])  # relu2_1 -> relu3_1\n",
    "enc_4 = nn.Sequential(*list(vgg.children())[18:31])  # relu3_1 -> relu4_1\n",
    "enc_5 = nn.Sequential(*list(vgg.children())[31:44])  # relu4_1 -> relu5_1\n",
    "\n",
    "norm.to(device)\n",
    "enc_1.to(device)\n",
    "enc_2.to(device)\n",
    "enc_3.to(device)\n",
    "enc_4.to(device)\n",
    "enc_5.to(device)\n",
    "transform.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1331, 1054])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
