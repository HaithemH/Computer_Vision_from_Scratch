{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os\n",
    "import time\n",
    "# import math\n",
    "import random\n",
    "import argparse\n",
    "from distutils.version import LooseVersion\n",
    "# Numerical libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Our libs\n",
    "from config import *\n",
    "from dataset import TrainDataset\n",
    "from models import models #ModelBuilder, SegmentationModule\n",
    "from models import *\n",
    "from utils import AverageMeter, parse_devices, setup_logger\n",
    "from lib.nn import UserScatteredDataParallel, user_scattered_collate, patch_replication_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6a58c50d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yacs.config import CfgNode as CN\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config definition\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "_C = CN()\n",
    "_C.DIR = \"ckpt/resnet50-upernet\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.DATASET = CN()\n",
    "_C.DATASET.root_dataset = \"./data/\"\n",
    "_C.DATASET.list_train = \"./data/training.odgt\"\n",
    "_C.DATASET.list_val = \"./data/validation.odgt\"\n",
    "_C.DATASET.num_class = 2\n",
    "# multiscale train/test, size of short edge (int or tuple)\n",
    "_C.DATASET.imgSizes = (300, 375, 450, 525, 600)\n",
    "# maximum input image size of long edge\n",
    "_C.DATASET.imgMaxSize = 1000\n",
    "# maxmimum downsampling rate of the network\n",
    "_C.DATASET.padding_constant = 8\n",
    "# downsampling rate of the segmentation label\n",
    "_C.DATASET.segm_downsampling_rate = 8\n",
    "# randomly horizontally flip images when train/test\n",
    "_C.DATASET.random_flip = True\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.MODEL = CN()\n",
    "# architecture of net_encoder\n",
    "_C.MODEL.arch_encoder = \"resnet50\"\n",
    "# architecture of net_decoder\n",
    "_C.MODEL.arch_decoder = \"upernet\"\n",
    "# weights to finetune net_encoder\n",
    "_C.MODEL.weights_encoder = \"\"\n",
    "# weights to finetune net_decoder\n",
    "_C.MODEL.weights_decoder = \"\"\n",
    "# number of feature channels between encoder and decoder\n",
    "_C.MODEL.fc_dim = 2048\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TRAIN = CN()\n",
    "_C.TRAIN.batch_size_per_gpu = 1\n",
    "# epochs to train for\n",
    "_C.TRAIN.num_epoch = 1\n",
    "# epoch to start training. useful if continue from a checkpoint\n",
    "_C.TRAIN.start_epoch = 0\n",
    "# iterations of each epoch (irrelevant to batch size)\n",
    "_C.TRAIN.epoch_iters = 22\n",
    "\n",
    "_C.TRAIN.optim = \"SGD\"\n",
    "_C.TRAIN.lr_encoder = 0.02\n",
    "_C.TRAIN.lr_decoder = 0.02\n",
    "# power in poly to drop LR\n",
    "_C.TRAIN.lr_pow = 0.9\n",
    "# momentum for sgd, beta1 for adam\n",
    "_C.TRAIN.beta1 = 0.9\n",
    "# weights regularizer\n",
    "_C.TRAIN.weight_decay = 1e-4\n",
    "# the weighting of deep supervision loss\n",
    "_C.TRAIN.deep_sup_scale = 0.4\n",
    "# fix bn params, only under finetuning\n",
    "_C.TRAIN.fix_bn = False\n",
    "# number of data loading workers\n",
    "_C.TRAIN.workers = 1\n",
    "\n",
    "# frequency to display\n",
    "_C.TRAIN.disp_iter = 20\n",
    "# manual seed\n",
    "_C.TRAIN.seed = 304\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Validation\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.VAL = CN()\n",
    "# currently only supports 1\n",
    "_C.VAL.batch_size = 1\n",
    "# output visualization during validation\n",
    "_C.VAL.visualize = False\n",
    "# the checkpoint to evaluate on\n",
    "_C.VAL.checkpoint = \"epoch_20.pth\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Testing\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.TEST = CN()\n",
    "# currently only supports 1\n",
    "_C.TEST.batch_size = 1\n",
    "# the checkpoint to test on\n",
    "_C.TEST.checkpoint = \"epoch_20.pth\"\n",
    "# folder to output visualization results\n",
    "_C.TEST.result = \"./\"\n",
    "\n",
    "\n",
    "cfg = _C\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"PyTorch Semantic Segmentation Training\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cfg\",\n",
    "    default=\"configuration/resnet50dilated-ppm_deepsup.yaml\",\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--gpus\",\n",
    "#     default=\"0-3\",\n",
    "#     help=\"gpus to use, e.g. 0-3 or 0,1,2,3\"\n",
    "# )\n",
    "parser.add_argument(\n",
    "    \"--gpus\",\n",
    "    default=\"0\",\n",
    "    help=\"gpus to use, e.g. 0-3 or 0,1,2,3\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    ")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# cfg.merge_from_file(args.cfg)\n",
    "cfg.merge_from_list(args.opts)\n",
    "\n",
    "if not os.path.isdir(cfg.DIR):\n",
    "    os.makedirs(cfg.DIR)\n",
    "\n",
    "with open(os.path.join(cfg.DIR, 'config.yaml'), 'w') as f:\n",
    "    f.write(\"{}\".format(cfg))\n",
    "\n",
    "# Start from checkpoint\n",
    "if cfg.TRAIN.start_epoch > 0:\n",
    "    cfg.MODEL.weights_encoder = os.path.join(\n",
    "        cfg.DIR, 'encoder_epoch_{}.pth'.format(cfg.TRAIN.start_epoch))\n",
    "    cfg.MODEL.weights_decoder = os.path.join(\n",
    "        cfg.DIR, 'decoder_epoch_{}.pth'.format(cfg.TRAIN.start_epoch))\n",
    "    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n",
    "        os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n",
    "\n",
    "# Parse gpu ids\n",
    "gpus = parse_devices(args.gpus)\n",
    "gpus = [x.replace('gpu', '') for x in gpus]\n",
    "gpus = [int(x) for x in gpus]\n",
    "num_gpus = len(gpus)\n",
    "cfg.TRAIN.batch_size = num_gpus * cfg.TRAIN.batch_size_per_gpu\n",
    "\n",
    "cfg.TRAIN.max_iters = cfg.TRAIN.epoch_iters * cfg.TRAIN.num_epoch\n",
    "cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder\n",
    "cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder\n",
    "\n",
    "random.seed(cfg.TRAIN.seed)\n",
    "torch.manual_seed(cfg.TRAIN.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 20210\n"
     ]
    }
   ],
   "source": [
    "# Dataset and Loader\n",
    "dataset_train = TrainDataset(\n",
    "    cfg.DATASET.root_dataset,\n",
    "    cfg.DATASET.list_train,\n",
    "    cfg.DATASET,\n",
    "    batch_per_gpu=cfg.TRAIN.batch_size_per_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=len(gpus),  # we have modified data_parallel\n",
    "        shuffle=False,  # we do not use this param\n",
    "        collate_fn=user_scattered_collate,\n",
    "        num_workers=cfg.TRAIN.workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_train = iter(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iterator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 304, 400])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['img_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 38, 50])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['seg_label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(a['img_data'][0, :, : , :].permute((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(a['seg_label'][0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(a['img_data'][1, :, : , :].permute((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(a['seg_label'][1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_encoder = models.ModelBuilder.build_encoder(\n",
    "        arch=cfg.MODEL.arch_encoder.lower(),\n",
    "        fc_dim=cfg.MODEL.fc_dim,\n",
    "        weights=cfg.MODEL.weights_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code = net_encoder(a['img_data'], return_feature_maps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 76, 100])\n",
      "torch.Size([1, 512, 38, 50])\n",
      "torch.Size([1, 1024, 19, 25])\n",
      "torch.Size([1, 2048, 10, 13])\n"
     ]
    }
   ],
   "source": [
    "print(code[0].shape)\n",
    "print(code[1].shape)\n",
    "print(code[2].shape)\n",
    "print(code[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_decoder = models.ModelBuilder.build_decoder(\n",
    "        arch=cfg.MODEL.arch_decoder.lower(),\n",
    "        fc_dim=cfg.MODEL.fc_dim,\n",
    "        num_class=cfg.DATASET.num_class,\n",
    "        weights=cfg.MODEL.weights_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = net_decoder(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 76, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `input0 == target0 && input2 == target1 && input3 == target2' failed. size mismatch (got input: 1x2x76x100, target: 1x38x50) at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\THNN/generic/SpatialClassNLLCriterion.c:61",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-73b43263f6bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seg_label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1824\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1826\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1827\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1828\u001b[0m         \u001b[1;31m# dim == 3 or dim > 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Assertion `input0 == target0 && input2 == target1 && input3 == target2' failed. size mismatch (got input: 1x2x76x100, target: 1x38x50) at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\THNN/generic/SpatialClassNLLCriterion.c:61"
     ]
    }
   ],
   "source": [
    "crit(decode, a['seg_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_module = models.SegmentationModule(net_encoder, net_decoder, crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segmentation_module.train(not cfg.TRAIN.fix_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net_encoder(a['img_data'], return_feature_maps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
